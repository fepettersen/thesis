\section{About}
The current version of the software (14.02.14) works in the following way. 
Various parameters are specified in a python-script which calls the program. 
After initialization, where anisotropic diffusion constant, initial condition and areas of combined solution are set up we start solving the equation (using the implicit BE scheme). 
At each time-step we call the Solve-method of the combine-class which in turn calls the solve-method of the PDE-solver, calculates the number of walkers by the conversion-rate, gives them a random position in proximity to the coarse-grained PDE-mesh-point we are looking at, and writes this position to a file. 
The random walk program is then called with some parameters. This program reads the file with walker-positions, advances some specified number of time-steps and writes the final positions to the same file it read. 
The main program now reads this file, maps the walker-positions back to the coarse-grained PDE-mesh and converts the spatial distribution of walkers to a concentration distribution. 
Here we are faced with a few choices with respect to the combination of the two solutions we have for the same area: 
\begin{itemize}
 \item Some form of least squares fitting could be done. A polynomial regression was developed, but tests indicate that it is not a good solution.
 \item Cubic spline interpolation might be slightly better seeing as we force the derivatives to be equal at the end-points. 
 Interpolation forces the fitted curve to take the measured values at the interpolation-points, and so we must chose only some of the points to be used by the interpolation in order for there to be a difference. Immediately we are faced with the problem of which points to use and which to throw away. If we consistently chose the points which are closest to the PDE-solution (and the end-points which must be included for smoothness) we might as well not include a random walk model, and vice versa.
 \item We could use only the result from the random walk model
 \item Some sort of average might also help us. There are many to chose from, both arithmetic and geometric and with different kinds of weighting.
\end{itemize}
Intuitively, I find it better to use either an average or only the random walk result. At present (14.02.14) the simplest arithmetic mean is implemented at each mesh-point.\\
After each time-step the program also writes the combined solution on the coarse-grained mesh to a unique file.\\
The python script also does some other more or less clever stuff at each run. This is described in the appendix as a form of debugging \ref{debugging}.

\subsection{Limitations}

As with any software there are limitations. The limitations discussed in this chapter will be with regard to physical problems, and not memory or CPU limitations which are described later. \\
Assuming the dendritic spines stuff will be the final application:\\
First of all, to specify a source function, one must program it in the Diffusion class (Diffusion::f()). This might be fixed in a later version using inheritance.\\
The geometry is, currently, limited to a square. Of course we can argue that we have scaled the size to a unit square, but as discussed in chapter \ref{} the actual geometry remains a unit square. 
Furthermore the same issue arises on the random walks. \\
As of now, it is unclear if 3d-modeling is supported. This is actually a smaller issue since there is usually little more to learn from switching from 2d to 3d. In any case, the extension to 3d is most likely not very hard, provided we use an array of slices from the cubic matrix describing the solution.

\section{Adaptivity}

There are two adaptive parts of the software. First of all, the number of walkers which depend on the concentration, or the solution the the PDE in the relevant area. This must change in order to keep physical meaning and give results. Without this adaptivity, the results would either be wrong, or the model would not make physical sense.\\
Since a diffusion process in general has rapid changes in the beginning where for example high frequency variations are dampened and very slow convergence to a steady state later, we have introduced a test of the amount of change between two subsequent time-steps. If this amount is smaller than some limit, we will increase the time-step.\\
\emph{This increase should be done in a more elegant manner(linearly?)}

\section{Computational cost}
This chapter will consider the expensive parts of the code and look at possible improvements.

\subsection{Memory}
The memory-expensive parts of the code include storing the decomposed matrices, and storing the random walkers. 
None of these pose any problems.

\subsection{CPU time}
The program as it stands now (v 1.$\infty$) uses the BE discretization and a highly specialized tridiagonal solver. 
The random walk-part of the software has been excreted as a stand-alone program which communicates with the main-program through a binary .xyz file containing the positions of all walkers in 3d space. This makes it easier to change solver, and implement more advanced solvers like the Direct Simulation Monte Carlo (DSMC) Molecular Dynamics (MD) simulator written by Anders Hafreager (see chapter \ref{}). 
There are three expensive operations in the algorithm as it stands now with the BE discretization using standard LU-decomposition.
\begin{itemize}
\item Random walks are expensive if there are many walkers. The number of calls to the random number generator follows eq. \ref{calls_to_ran} and for the verification process, which required a lot of walkers, this represented a considerable cost. 
As an additional problem we will encounter some overhead upon calling the program, initializing variables and instances and so on. 
The computationally most demanding function seems to be the round-off function, which is used to place the walkers on the coarse-grained PDE-mesh.
\item Communicating the positions of all the walkers between the two programs each time-step is very costly.
\item Translating the positions of the walkers from the unit-square they are walking on to the coarse-grained PDE grid requires calling the ``round'' function from the math library in C++. This function is rather slow, and the program suffers from it. 
\end{itemize}

\begin{equation}\label{calls_to_ran}
 N_{\text{calls}} \propto Hc\frac{(x_1-x_0)}{\Delta x}\frac{(y_1-y_0)}{\Delta y}\tilde{T}
\end{equation}
where $\tilde{T}$ denotes the number of timesteps on the PDE level times the number of time-steps one PDE-step corresponds to on the RW level. Note that this expression will NOT be zero in 1d, and that it is dependent on the PDE-solution.

\subsection{Parallelizability}
In the final algorithm there are the following stages
\begin{itemize}
 \item Initialization\\
 Read parameters from command-line, initial condition and diffusion ``tensor'' from file. Setup instances of solvers etc. Practically no point in parallelizing this.
 \item LU-decomposition\\
 The actual LU-decomposition is a sort of Gaussian Elimination which is costly ($\mathcal{O}(N^3)$). Although the decomposition is pre-implemented at this point, the plan is to implement my own version of this. This step should be possible to parallelize.
 \item Solving\\
 This step includes a back-transform of the decomposed matrix which is expensive for $d>1$. This step should be parallelizable. It also includes the random walk part which is both expensive (depending on the number of walkers left) and highly parallelizable. We also write stuff to file which is quite costly. This is probably not possible to parallelize.
\end{itemize}

Parallelization of the random walk solver should scale linearly because the only form of communication required is shared memory. 
The LU decomposition and back-substitution require some communication and will not scale linearly, but will still benefit from parallelizing. 

\subsection{Some fancy title about changing stuff}

It should be rather simple to replace parts of the code as long as certain conditions are met. Perhaps the simplest part to replace is the random walk part. 
Requirements for this part are:
\begin{itemize}
 \item Locating executable main-file of the program in the folder ``stochastic'' and naming it ``walk\_solver''. 
 \item This executable should read the filename of the ini-file containing the positions of all walkers , and the local diffusion ``tensor'' in the relevant area.
 \item Upon completion, all positions must be written to the same file.
\end{itemize}
The PDE-solver should also be rather simple to replace, but some more programming will be required. First of all your solver must be included in the header-file. 
Next, the ``Combine'' class has an instance of the PDE-solver which it calls the advance-method of at each time-step. Seeing as this method is most regularly named ``solve'' you will either have to rename the method or the call. 
There are really very few dependencies on the PDE-solver, seeing as it is mostly left alone, but in addition to being able to respond to function calls it must:
\begin{itemize}
 \item Have its own $\Delta t$ attribute named ``dt''.
 \item Work on ``double**'' data types for all spatial dimensions (or implement some form of workaround)
 \item Be able to respond to increasing the time-step. In practice this means that the solver should be implicit.
\end{itemize}
As discussed, the implementation of random walks on parts of the mesh will reduce the convergence-rate to 0.5, and so there are really only two reasons to implement a new PDE solver. 
The current one only implements Neumann boundaries, and consequently must either be modified or replaced in order to work with other boundary conditions.
It also only works on a square mesh. As discussed in section \ref{geometry} it will be immensely complicated to implement a grid transformation, and this is already implemented in most finite element software.

\section{Model problem}

