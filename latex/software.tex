\section{About}

\section{Adaptivity}
There are two adaptive parts of the software. First of all, the number of walkers which depend on the concentration, or the solution the the PDE in the relevant area. This must change in order to keep physical meaning and give results. Without this adaptivity, the results would either be wrong, or the model would not make physical sense.\\
Since a diffusion process in general has rapid changes in the beginning where for example high frequency variations are dampened and very slow convergence to a steady state later, we have introduced a test of the amount of change between two subsequent time-steps. If this amount is smaller than some limit, we will increase the time-step.\\
\emph{This increase should be done in a more elegant manner(linearly?)}

\section{Computational cost}
This chapter will consider the expensive parts of the code and look at possible improvements.

\subsection{Memory}
The memory-expensive parts of the code include storing the decomposed matrices, and storing the random walkers. 
None of these pose any problems.

\subsection{CPU time}
The program as it stands now (v 1.$\infty$) uses the BE discretization and a highly specialized tridiagonal solver. 
The random walk-part of the software has been excreted as a stand-alone program which communicates with the main-program through a binary .xyz file containing the positions of all walkers in 3d space. This makes it easier to change solver, and implement more advanced solvers like the Direct Simulation Monte Carlo (DSMC) Molecular Dynamics (MD) simulator written by Anders Hafreager (see chapter \ref{}). 
There are three expensive operations in the algorithm as it stands now with the BE discretization using standard LU-decomposition.
\begin{itemize}
\item Random walks are expensive if there are many walkers. The number of calls to the random number generator follows eq. \ref{calls_to_ran} and for the verification process, which required a lot of walkers, this represented a considerable cost. 
As an additional problem we will encounter some overhead upon calling the program, initializing variables and instances and so on. 
The computationally most demanding function seems to be the round-off function, which is used to place the walkers on the coarse-grained PDE-mesh.
\item Communicating the positions of all the walkers between the two programs each time-step is very costly.
\item Translating the positions of the walkers from the unit-square they are walking on to the coarse-grained PDE grid requires calling the ``round'' function from the math library in C++. This function is rather slow, and the program suffers from it. 
\end{itemize}

\begin{equation}\label{calls_to_ran}
 N_{\text{calls}} \propto Hc\frac{(x_1-x_0)}{\Delta x}\frac{(y_1-y_0)}{\Delta y}\tilde{T}
\end{equation}
where $\tilde{T}$ denotes the number of timesteps on the PDE level times the number of time-steps one PDE-step corresponds to on the RW level. Note that this expression will NOT be zero in 1d, and that it is dependent on the PDE-solution.

\subsection{Parallelizability}
In the final algorithm there are the following stages
\begin{itemize}
 \item Initialization\\
 Read parameters from command-line, initial condition and diffusion ``tensor'' from file. Setup instances of solvers etc. Practically no point in parallelizing this.
 \item LU-decomposition\\
 The actual LU-decomposition is a sort of Gaussian Elimination which is costly ($\mathcal{O}(N^3)$). Although the decomposition is pre-implemented at this point, the plan is to implement my own version of this. This step should be possible to parallelize.
 \item Solving\\
 This step includes a back-transform of the decomposed matrix which is expensive for $d>1$. This step should be parallelizable. It also includes the random walk part which is both expensive (depending on the number of walkers left) and highly parallelizable. We also write stuff to file which is quite costly. This is probably not possible to parallelize.
\end{itemize}

Parallelization of the random walk solver should scale linearly because the only form of communication required is shared memory. 
The LU decomposition and back-substitution require some communication and will not scale linearly, but will still benefit from parallelizing. 

\subsection{Some fancy title about changing stuff}

It should be rather simple to replace parts of the code as long as certain conditions are met. Perhaps the simplest part to replace is the random walk part. 
Requirements for this part are:
\begin{itemize}
 \item Locating executable main-file of the program in the folder ``stochastic'' and naming it ``walk\_solver''. 
 \item This executable should read the filename of the ini-file containing the positions of all walkers , and the local diffusion ``tensor'' in the relevant area.
 \item Upon completion, all positions must be written to the same file.
\end{itemize}
The PDE-solver should also be rather simple to replace, but some more programming will be required. First of all your solver must be included in the header-file. 
Next, the ``Combine'' class has an instance of the PDE-solver which it calls the advance-method of at each time-step. Seeing as this method is most regularly named ``solve'' you will either have to rename the method or the call. 
There are really very few dependencies on the PDE-solver, seeing as it is mostly left alone, but in addition to being able to respond to function calls it must:
\begin{itemize}
 \item Have its own $\Delta t$ attribute named ``dt''.
 \item Work on ``double**'' data types for all spatial dimensions (or implement some form of workaround)
 \item Be able to respond to increasing the time-step. In practice this means that the solver should be implicit.
\end{itemize}
As discussed, the implementation of random walks on parts of the mesh will reduce the convergence-rate to 0.5, and so there are really only two reasons to implement a new PDE solver. 
The current one only implements Neumann boundaries, and consequently must either be modified or replaced in order to work with other boundary conditions.
It also only works on a square mesh. As discussed in section \ref{} it will be immensely complicated to implement a grid transformation, and this is already implemented in most finite element software.