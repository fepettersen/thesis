
In this chapter we will take a closer look at random walks, both in general and the transition from the statistical view to partial differential equations. 
We will take a look at different algorithms to produce random walks, and discuss their pros and cons in light of this project. 
Then we will take a quick look at partial differential equations and numerical solution of them.

\section{Introduction to random walks}\label{introduction_to_random_walks}
The most basic random walk is a walker on the x-axis which will take a step of a fixed length to the right with a probability $p$, or to the left with a probability $q=1-p$. 
Using (pseudo-) random numbers on a computer we can simulate the outcomes of a random walk. 
For each step (of which there are N) we draw a random number, r, between 0 and 1 from some distribution (say a uniform one) which will be the probability. 
If $r\leq p$ the walker will take a step to the left, otherwise it will take a step to the right. 
After the N steps the walker will have taken R steps to the right, and $L = N-R$ steps to the left. 
The net displacement from the origin will be $S = R-L$. \\
This simple approach is easily generalizable to two and three dimensions by having $2d$ possible outcomes from the random number, where d is the dimensionality. 
In two dimensions the walker will step up if $r\in(0.75,1]$ and left if $r\in[0,0.25)$, for example.

\subsection{Further discussion and analysis of the introduction}\label{further_introduction}
The following derivation is borrowed from a compendium in statistical mechanics by Finn Ravndal. \\
If we do sufficiently many walks, the net displacement will vary from $S=+N$ to $S=-N$ representing all steps to the right and all steps to the left respectively. 
The probability of all steps being to the right is $P_N(N) = p^N$. 
Should one of the steps be to the left, and the rest to the right we will get a net displacement of $S = N-2$ with the probability $P_N(R = N-1) = Np^{N-1}q$. 
We can generalize this to finding the probability of a walk with a R steps to the right as 
\begin{equation}\label{bernoulli_distr}
 P_N(R) = {N\choose R}p^{R}q^{N-R}
\end{equation}
where ${N\choose R}=\frac{N!}{R!(N-R)!}$ is the number of walks which satisfy the net displacement in question, or the multiplicity of this walk in statistical mechanics terms. 
Equation \ref{bernoulli_distr} is the Bernoulli probability distribution, which is normalized.
\begin{align*}
 \sum\limits_{R=0}^N P_N(R) = (p+q)^N = 1^N = 1
\end{align*}
We can use this distribution to calculate various average properties of a walk consisting of N steps. 
For example, the average number of steps to the right is
\begin{align*}
 \langle R\rangle &=  \sum\limits_{R=0}^N RP_N(R) =  \sum\limits_{R=0}^N {N\choose R}Rp^Rq^{N-R} = \\
 p\frac{d}{dp} \sum\limits_{R=0}^N {N\choose R}p^Rq^{N-R} &= p\frac{d}{dp}(p+q)^N = Np(p+q)^{N-1} = Np
\end{align*}
From this we can also find the average value of the net displacement using $S = R-L = R-(N-R) = 2R-N$.
\begin{align*}
 \langle S\rangle = \langle2R\rangle -N = 2Np-N(p+q) = N(2p-p-q) = N(p-q)
\end{align*}
We notice that the average net displacement is greatly dependent on the relationship between $p$ and $q$, and that any symmetric walk will have an expected net displacement of zero. 
In many cases we will be more interested in the mean square displacement than the displacement itself, because many important large scale parameters can be related to the root-mean-square displacement. 
This can also be calculated rather straightforwardly. 
\begin{align*}
  \langle R^2\rangle =  \sum\limits_{R=0}^N R^2P_N(R) &=  \sum\limits_{R=0}^N {N\choose R}R^2p^Rq^{N-R} = \\
 \left(p\frac{d}{dp}\right)^2 \sum\limits_{R=0}^N {N\choose R}p^Rq^{N-R} &= \left(p\frac{d}{dp}\right)^2(p+q)^N \\
 = Np(p+q)^{N-1} +p^2N(N-1)(p+q)^{N-2} &= (Np)^2 +Np(1-p) = (Np)^2 +Npq
\end{align*}
Like before, the average net displacement is given as $S^2 = (2R-N)^2$ and we obtain
\begin{align*}
 \langle S^2\rangle = 4\langle R^2\rangle -4N\langle R\rangle + N^2 &= 4((Np)^2 +Npq) -4N^2p + N^2\\
 = N^2(4p^2 -4p +1) +4Npq &= N^2(2p-1)^2 +4Npq = N^2(p-q)^2 +4Npq.
\end{align*}
which for the 1D symmetric walk gives $\langle S^2\rangle =N$ and the variance, denoted $\langle\Delta S^2\rangle = \langle\langle S^2\rangle-\langle S\rangle^2\rangle$, is found by insertion as
\begin{align}
 \langle\Delta S^2\rangle &= \langle N^2(p-q)^2 +4Npq - ( N(p-q))^2\rangle= 4Npq 
\end{align}

When the number of steps gets very large we can approximate the Bernoulli distribution (eq. \ref{bernoulli_distr}) by the Gaussian distribution. 
This is most easily done in the symmetric case where $p=q=\frac{1}{2}$, but it is sufficient for the steplengths to have a finite variance (\emph{find something to refer to}). 
The Bernoulli distribution then simplifies to
\begin{equation}
 P(S,N) = \left(\frac{1}{2}\right)^N\frac{N!}{R!L!}
\end{equation}
on which we apply Stirling's famous formula for large factorials $n!\simeq\sqrt{2\pi n}\cdot n^ne^{-n}$.
\begin{align*}
 P(S,N) &= \left(\frac{1}{2}\right)^N\frac{N!}{R!L!} \\
 &= \exp\left(-N\ln2+\ln\sqrt{2\pi N}+N\ln N - \ln\sqrt{2\pi R} -R\ln R - \ln\sqrt{2\pi L} - L\ln L \right) \\
 &= \sqrt{\frac{N}{2\pi RL}}\exp\left(-R\ln\frac{2R}{N}-L\ln\frac{2L}{N}\right)
\end{align*}
Where we have used $R+L=N$. We now insert for $\frac{2R}{N}=1+\frac{S}{N}$ and $\frac{2L}{N}=1-\frac{S}{N}$ and expand the logarithms to first order, $RL=\frac{N^2-S^2}{4}$ in the prefactor, and approximate $1-\frac{S^2}{N^2}\simeq1$. This gives
\begin{equation}\label{descrete_gaussian_distr}
 P(S,N) =\sqrt{\frac{2}{\pi N}}\exp\left(\frac{-S^2}{2N}\right)
\end{equation}
which is an ordinary, discrete Gaussian distribution with $\langle S\rangle = 0$  and $\langle S^2\rangle = N$. 
If we keep assuming that the walker is on the x-axis, and let the step length, a, get small the final position will be $x=Sa$ which we can assume is a continuous variable. 
Similarly, we let the time interval between each step, $\tau$, be small and let the walk run for a continuous time $t=N\tau$. This changes the distribution \ref{descrete_gaussian_distr} to
\begin{equation}
 P(x,t) = \frac{1}{2a}\sqrt{\frac{2\tau}{\pi t}}\exp\left(-\frac{x^2\tau}{2a^2t}\right). 
\end{equation}
The prefactor $\frac{1}{2a}$ is needed to normalize the continuous probability distribution since the separation between each possible final position in walks with the same number of steps is $\Delta x=2a$. 
We also introduce the diffusion constant
\begin{equation}
D = \frac{a^2}{2\tau} 
\end{equation}
making the distribution
\begin{equation}
 P(x,t) = \sqrt{\frac{1}{4\pi Dt}}\exp\left(-\frac{x^2}{4Dt}\right)
\end{equation}

Introducing $x$ also gives us the expectation value and variance of x on a form which will be useful later. 

We have $x=Sa$ which means 
$$\langle x\rangle=a\langle S\rangle$$ 
and 
$$\langle x^2\rangle=a^2\langle S^2\rangle$$
Finally by insertion we find the variance $\langle \Delta x^2\rangle$
\begin{equation}\label{random_walk_variance}
 \langle \Delta x^2\rangle = \langle\langle x^2\rangle -\langle x\rangle^2\rangle = \langle a^2\langle S^2\rangle -a^2\langle S\rangle^2\rangle = 4Npqa^2
\end{equation}


\subsection{More general Random Walks}\label{more_general_random_walks}

In the more general case, the position of a random walker, $\vec{r}$ at a time $t_i$ is given by the sum
\begin{equation}\label{brownian_motion}
 \vec{r}(t_i)=\sum\limits_{j=0}^i \Delta \vec{x}(t_j)
\end{equation}
where $\Delta \vec{x}(t_j) = \left(\Delta x(t_j),\Delta y(t_j),\Delta z(t_j)\right)$ in 3D. Each $\Delta x,\Delta y,\Delta z$ is a random number drawn from a distribution with a finite variance $\sigma^2 = \langle\Delta x^2\rangle$. 
By the central limit theorem, any stochastic process with a well defined mean and variance can, given enough samples, be approximated by a Gaussian distribution. 
This means that the probability of finding the walker at some position x after M steps is 
\begin{equation}
 P(x,M)\propto e^{-\frac{x^2}{2M\sigma^2}}
\end{equation}
Remember that the actual Gaussian distribution is 
$$
\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{(n-\mu)^2}{2\sigma^2}\right)
$$

We can introduce an Einstein relation $\sigma^2 = 2dD\Delta t$ and the obvious relation $t = M\Delta t$ to get a more desirable exponent.
We see that $\langle \Delta x^2\rangle = 2Dt$ in the one dimensional case where $d=1$. 

\emph{The introduction of the Einstein relation might put some restrictions on our model.}
Normalizing the expression gives us 
\begin{equation}\label{rw_gaussian_distribution}
 P(x,t) = \sqrt{\frac{1}{4Dt}}\exp\left(-\frac{x^2}{4Dt}\right)
\end{equation}

If we have a large number, N, of walkers, their concentration will be $C(x,t) = NP(x,t)$. 
The concentration is conserved, so any amount that flows out of an area must reflect as a decrease in concentration. 
We can express this by the flow of concentration
\begin{equation}
 \frac{\d C}{\d t} -\nabla\cdot\vec{J} = S
\end{equation}
where $\vec{J}$ is the flow vector and S is a source term which in our case will be zero.
From Fick's first law we know that $\vec{J} = -D\nabla C$. Inserting this gives us
\begin{equation}\label{simple_diffusion_equation}
 \frac{\d C}{\d t} = \nabla\cdot \left(D\cdot\nabla C\right)
\end{equation}
which is the diffusion equation.
By insertion we can check that this version (\ref{rw_gaussian_distribution}) of the Gaussian distribution fulfills the diffusion equation. 
Starting with only the time derivative gives us
\begin{align*}
 \frac{\d P}{\d t} = -\frac{4\pi D\exp\left(-\frac{x^2}{4Dt}\right)}{2\sqrt{(4\pi Dt)^3}}&+\frac{x^2\exp\left(-\frac{x^2}{4Dt}\right)}{4Dt^2\sqrt{4\pi Dt}} \\
 = \exp\left(-\frac{x^2}{4Dt}\right)\left(\frac{8Dx^2}{2\sqrt{\pi}(4Dt)^{5/2}} -\frac{(4D)^2t}{2\sqrt{\pi}(4Dt)^{5/2}}\right)&= \frac{4D\exp\left(-\frac{x^2}{4Dt}\right)(x^2-2Dt)}{\sqrt{\pi}(4Dt)^{5/2}}
\end{align*}
 We then finish by doing the spatial derivative
\begin{align*}
 D\frac{\d^2 P}{\d x^2} &= \frac{D}{\sqrt{4\pi Dt}}\frac{\d}{\d x}\left[-\exp\left(\frac{-x^2}{4Dt}\right)\left(\frac{-2x}{4Dt}\right)\right]\\
 = \frac{2D}{4Dt\sqrt{4\pi Dt}}&\exp\left(\frac{-x^2}{4Dt}\right)\left[1-x\left(\frac{2x}{4Dt}\right)\right] = \frac{4D\exp\left(-\frac{x^2}{4Dt}\right)(x^2-2Dt)}{\sqrt{\pi}(4Dt)^{5/2}}
\end{align*}
and see that they are equal, meaning that the diffusion equation is satisfied.

\subsection{Choosing random walk algorithm}\label{choosing_random_walk_algorithm}

The simplest random walk model, which places walkers on discrete mesh points and uses a fixed step length, has been used with great success to model diffusion processes. 
Farnell and Gibson discuss this in their article \cite{farnell2005monte}. 
In this project we will be torn between chosing a realistic algorithm to advance the random walkers, like brownian motion or to go for simplicity. 
That being said, by the central limit theorem both models will after some timesteps be described by a Gaussian distribution meaning that on the PDE scale we will not know the difference. 
Hence it will make no sense to not use the simplest random walk model.
We should note that it will be quite easy to change the algorithm used for random walks, and so we have not locked ourselves to anything yet.

\subsection{Random walks and anisotropy}\label{random_walks_and_anisotropy}

Any real problem where parts of the diffusion process cannot be modelled by the continuum approximation is bound to be anisotropic. 
There is reason to believe that an anisotropic diffusion process on the PDE level will lead to an anisotropic random walk model as well, but how do we model this. 
If we simply replace the diffusion constant by a function $D = D(\vec{x})$ (see eq \ref{steplength}) we are at least started, but this will not quite be sufficient as Farnell and Gibson point out \cite{farnell2005monte}. 
Through their experiments they found that only adjusting the steplength will not improve the error noticeably and reasoned that this is because the walkers are still as likely to jump in both directions (right or left in 1d), and that the stepsize is the same in both cases, hence the model does not resemble anisotropy. 
They went on to introduce an adjusted steplength and an adjusted step probability, a solution they landed on after trial and error. 
The expressions they proposed are listed in equations \ref{farnell_and_gibson_1} to \ref{farnell_and_gibson_5}. 
\begin{equation}\label{farnell_and_gibson_1}
 \Delta_p(x) = \frac{1}{2}\left(L(x) + L(x +\Delta_p(x))\right) \to L(x) +\frac{1}{2}L(x)L'(x)
\end{equation}
\begin{equation}\label{farnell_and_gibson_2}
 \Delta_m(x) = \frac{1}{2}\left(L(x) + L(x -\Delta_m(x))\right) \to L(x) +\frac{1}{2}L(x)L'(x)
\end{equation}
where $L(x)$ is defined in equation \ref{farnell_and_gibson_3} and $\Delta_p(x)$ and $\Delta_m(x)$ are the adjusted steplengths to the right and left, respectively.
\begin{equation}\label{farnell_and_gibson_3}
L(x) = \sqrt{2D(x)\Delta t}
\end{equation}
We also have the adjusted jump probabilities $T_r(x)$ and $T_l(x)$ which are the probabilities for a walker at position x to jump right or left, respectively. 
These are defined in equations \ref{farnell_and_gibson_4} and \ref{farnell_and_gibson_5}
\begin{equation}\label{farnell_and_gibson_4}
T_r(x) = \frac{1}{2} +\frac{1}{4}L'(x)
\end{equation}
\begin{equation}\label{farnell_and_gibson_5}
T_l(x) = \frac{1}{2} -\frac{1}{4}L'(x)
\end{equation}
We notice that the adjusted steplength we proposed to start with is still a part of the final expressions.


\subsection{Random walks and drift}\label{random_walks_and_drift}

Another point we have yet to say something about is diffusion that has a drift term, $\frac{\d u}{\d x}$. 

Initially one thought that diffusion in the Extra Cellular Space of the brain was governed by a drift term, but the modern perception is that this drift term is in the very least negligible \cite{nicholson2001diffusion}. 
Though it is unlikely that we will include a drift term in our model, we will say a few words about it here since it is of importance in other applications and might be a natural extension at some point, should someone else use this work.\\
We model random walkers with drift by simply adding some vector to the Brownian motion model, thus forcing all walkers to have a tendency to walk a certain direction. 
This approach can also be used in the fixed steplength (or variable steplength in the anisotropic case)  if we express the new step, $\vec{s}$, as
\begin{align*}
 \vec{s} = (\pm l \text{ or }0,\pm l \text{ or }0) +\vec{d}
\end{align*}
where $\vec{d}$ denotes the drift of the walker.\\
We can set up the continuity equation for a concentration, $C(x,t) = NP(x,t)$ of random walkers which are affected by a drift.
\begin{equation}
 \frac{\d C}{\d t} +\nabla\cdot\vec{j} = S
\end{equation}
Where $\vec{j}$ denotes the total flux of walkers through some enclosed volume and $S$ is a source/sink term. 
Since the walkers are affected by drift the flux will consist of two terms; $\vec{j} = \vec{j}_{diff}+\vec{j}_{drift}$. 
From Fick's first law we know that $\vec{j}_{diff} = -D\nabla C$. 
The second flux term is the advective flux which will be equal to the average velocity of the system; $\vec{j}_{drift} = \vec{v}C$. 
Inserting this in the continuity equation gives us the well known convection diffusion equation (\ref{convection_diffusion_equation}).
\begin{equation}\label{convection_diffusion_equation}
 \frac{\d C}{\d t} = \nabla\cdot\left(D\nabla C\right)-\nabla\cdot\left(\vec{v}C\right) + S
\end{equation}
Which in many cases will simplify to
\begin{equation}
 \frac{\d C}{\d t} = D\nabla^2 C-\vec{v}\cdot\nabla C
\end{equation}

In order to determine all the parameters of the convection diffusion equation \ref{convection_diffusion_equation} we will need to go through some of the calculations from chapter \ref{introduction_to_random_walks}. 
The situation is the same, a walker in one dimension which can jump left or right, but this time will also move a finite distance $d$ each timestep. 
This will make the expected net displacement
\begin{equation*}
 \langle S\rangle = R-L +Nd = N(p-q) + Nd
\end{equation*}
and the expected mean square displacement
\begin{equation*}
 \langle S^2\rangle = (2\langle R\rangle -N)^2 +(Nd)^2 = N^2(p-q)^2 +4Npq +(Nd)^2
\end{equation*}
which in turn gives us the variance
\begin{align*}
 \langle \Delta S^2\rangle &= \langle\langle S^2\rangle-\langle S\rangle^2\rangle \\
 &= N^2(p-q)^2 +4Npq +(Nd)^2 - N^2(p-q)^2 -(Nd)^2 \\
\langle \Delta S^2\rangle &= 4Npq
\end{align*}
This shows us that the variance is untouched by the drift term, but not the mean which for the symmetric case is $\langle S\rangle = Nd$. 
When we convert this to the continuous variables $x$ and $t$ we get the solution shown in equation \ref{solution_convection_diffusion_eq}.
\begin{equation}\label{solution_convection_diffusion_eq}
 C(x,t) = \frac{N}{\sqrt{4\pi Dt}}\exp\left(-\frac{(x-vt)^2}{4Dt}\right)
\end{equation}
Where $v = \frac{d}{\Delta t}$ is the velocity of the concentration and $D$ is the well known diffusion constant, inserted from the Einstein relation $\sigma^2 = 2D\Delta t$.\\

%%%%%% Not sure if this should be included %%%%%%%%%

% The only problem with the solution \ref{solution_convection_diffusion_eq} is that it is invalid for $t=0$. 
% In order to use it we will need an initial condition which fits the rest of the solution. 
% We try and find this by first checking by simulation if it converges towards some initial solution as $t\to0$ and if so, extrapolate this to $t=0$ as the initial condition. 
% Figure \ref{convection_diffusion_eq_initial_condition} shows the result of this little experiment, and as we clearly see it does diverge as $t\to0$. 
% However, it also converges to a Dirac delta function, $\delta(x)$ which is defined by its properties
% \begin{align*}
% \delta(x) = \begin{cases} +\infty, & x = 0 \\ 0, & x \ne 0 \end{cases}
% \end{align*}
% and
% \begin{equation*}
%  \int_{-\infty}^\infty \delta(x) \, dx = 1
% \end{equation*}
% The Dirac delta function often sought as an initial condition in experimental setups for measurements on diffusion processes because of its compatibility with a variety of diffusion equations, and should do a very good job in our numerical setup as well. 
% 
% \begin{figure}[H]
%  \centering
%  \includegraphics[scale=0.7]{Figures/convection_diffusion_eq_initial_condition.eps}
%  \caption[Initial condition for convection diffusion]{Experiment to determine the initial condition of equation \ref{solution_convection_diffusion_eq}}
%  \label{convection_diffusion_eq_initial_condition}
% \end{figure}


\section{Some words about partial differential equations}\label{some_words_on_PDEs}
\subsection{Discretizing}\label{discretizing}

To maintain a bit of generality we will look at the (potentially) anisotropic diffusion equation in 2d. The extension to 3d is trivial, as is the 1d version.
\begin{equation}
 \frac{\d u}{\d t} = \nabla D\nabla u +f
\end{equation}
where f is some source term. 
The final expression and scheme will depend on how we chose to approximate the time derivative, but the spatial derivative will have the same approximation. \\
We start off by doing the innermost derivative in one dimension. 
The generalization to more dimensions is trivial, and will consist of adding the same terms for the y and z derivatives. 
\begin{align*}
 \left[\frac{d}{dx}u\right]^n \approx \frac{u^n_{i+1/2}-u^n_{i-1/2}}{\Delta x}
\end{align*}
Where we have made the approximate derivative around the point $x_i$. 
We then set $\phi(x)=D\frac{du}{dx}$ and do the second derivative
\begin{align*}
  \left[\frac{d}{dx}\phi\right]^n \approx \frac{\phi^n_{i+1/2}-\phi^n_{i-1/2}}{\Delta x}
\end{align*}
and insert for $\phi$
\begin{align*}
 \frac{\phi^n_{i+1/2}-\phi^n_{i-1/2}}{\Delta x} = \frac{1}{\Delta x^2}\left(D_{i+1/2}(u^n_{i+1}-u^n_{i+1}) -D_{i-1/2}(u^n_{i}-u^n_{i-1})\right)
\end{align*}
Since we can only evaluate the diffusion constant at the mesh points (or strictly speaking since it is a lot simpler to do so) we must approximate $D_{i\pm1/2}\approx0.5(D_{i\pm1}+D_i)$. 
Inserting this gives us 
\begin{align*}
 \nabla D\nabla u\approx\frac{1}{2\Delta x^2}\left((D_{i+1,j}+D_{i,j})(u_{i+1,j}-u_{i,j})-(D_{i,j}+D_{i-1,j})(u_{i,j}-u_{i-1,j})\right) \\
 +\frac{1}{2\Delta y^2}\left((D_{i,j+1}+D_{i,j})(u_{i,j+1}-u_{i,j})-(D_{i,j}+D_{i,j-1})(u_{i,j}-u_{i,j-1})\right)
\end{align*}

The discretization of the time-derivative is where we can see a difference between the two schemes we will use in this project. 
When ordinary differential equations are discretized one can clearly see how this difference arises, and so we will write our PDE \nomenclature{PDE}{Parital Differential Equation} using a new notation
\begin{equation}
 D_t u = Ou
\end{equation}
where the right-hand-side operator $O$ indicates some operation like the double spatial derivative. 
Introducing the general discretization of the time derivative gives us equation \ref{theta_rule} known as theta-rule. 
Setting $\theta = 0$ yields the FE\nomenclature{FE}{Forward Euler} discretization, and $\theta = 1$ the BE \nomenclature{BE}{Backward Euler} discretization. 

\begin{equation}\label{theta_rule}
 \frac{u^{n+1}-u^n}{\Delta t} = O\left(\theta u^{n+1} +(1-\theta)u^n\right)
\end{equation}

From the theta rule we can see that the only difference between the FE and BE scheme is at what time-step we evaluate the right-hand-side of the equation. 
The theta-rule can give other schemes as well, using some weighted average of the right-hand-side at $t^n$ and $t^{n+1}$ but we will not look into these in this project. 
We can now summarize by writing out the FE discretization as it will be implemented in 1d in equation \ref{FE_scheme_1D}.

\begin{align}\label{FE_scheme_1D}
 u^{n+1}_i = \frac{\Delta t}{2\Delta x^2}\left((D_{i+1}+D_{i})(u^n_{i+1}-u^n_{i})-(D_{i}+D_{i-1})(u^n_{i}-u^n_{i-1})\right) + u^n_i
\end{align}

We will come back to the FE discretization when we discuss stability later. \\
Looking at the BE discretization which is written out in 1d in equation \ref{BE_scheme_1D} we notice that there are quite a few more unknowns per mesh-point. 

\begin{align}\label{BE_scheme_1D}
 u^{n+1}_i = \frac{\Delta t}{2\Delta x^2}\left((D_{i+1}+D_{i})(u^{n+1}_{i+1}-u^{n+1}_{i})-(D_{i}+D_{i-1})(u^{n+1}_{i}-u^{n+1}_{i-1})\right) + u^n_i
\end{align}

Writing out the calculations for a small mesh we recognize a pattern which we can exploit.
\begin{align*}
 &u^{n+1}_0 =  \frac{\Delta t}{2\Delta x^2}\left(2(D_{0}+D_{1})(u^{n+1}_{1}-u^{n+1}_{0})\right) + u^n_0\\
 &u^{n+1}_1 = \frac{\Delta t}{2\Delta x^2}\left((D_{2}+D_{1})(u^{n+1}_{2}-u^{n+1}_{1})-(D_{1}+D_{0})(u^{n+1}_{1}-u^{n+1}_{0})\right) + u^n_1\\
 &u^{n+1}_2 = \frac{\Delta t}{2\Delta x^2}\left((D_{3}+D_{2})(u^{n+1}_{3}-u^{n+1}_{2})-(D_{2}+D_{1})(u^{n+1}_{2}-u^{n+1}_{1})\right) + u^n_2 \\
 &u^{n+1}_3 =  \frac{\Delta t}{2\Delta x^2}\left(2(D_{2}+D_{3})(u^{n+1}_{3}-u^{n+1}_{2})\right) + u^n_3
\end{align*}

Rearranging this and setting $a = \frac{\Delta t}{2\Delta x^2}$ gives us a normal system of linear equations
\begin{align*}
 &u^{n+1}_0\left(1+2a(D_0+D_1)\right)- 2au^{n+1}_{1}(D_1+D_0) =  u^n_0\\
 &u^{n+1}_1\left(1+a(D_2+2D_1+D_0)\right)-au^{n+1}_{2}(D_2+D_1)-au^{n+1}_{0}(D_1+D_0) = u^n_1\\
 &u^{n+1}_2\left(1+a(D_3+2D_2+D_1)\right)-au^{n+1}_{3}(D_3+D_2)-au^{n+1}_{1}(D_2+D_1) = u^n_2\\
 &u^{n+1}_3\left(1+2a(D_3+D_2)\right)- 2au^{n+1}_{2}(D_3+D_2) =  u^n_3
\end{align*}
which we arrange as 
{\tiny
\begin{align}\label{BE}
 \left(\begin{array}{c c c c}
        1+2a(D_0+D_1) & -2a(D_1+D_0) &0 &0 \\
        -a(D_1+D_0) &1+a(D_2+2D_1+D_0) & -a(D_2+D_1) &0 \\
        0& -a(D_2+D_1) & 1+a(D_3+2D_2+D_1)& -a(D_3+D_2)\\
        0& 0& 1+2a(D_3+D_2) & - 2a(D_3+D_2)\\
       \end{array}\right)\mathbf{u}^{n+1} = \mathbf{u}^{n} \\
 \mathbf{A}\mathbf{u}^n = \mathbf{u}^{n-1}
\end{align}
}
Immediately we notice a problem with the implicit scheme. If we solve the system of equations by the fool-proof Gaussian elimination we will use some $\mathcal O(n^3)$ FLOPs per time-step. This will get even worse in more spatial dimensions; $\mathcal O(n^6)$ in 2d and $\mathcal O(n^9)$ in 3d. 
As a comparison the explicit scheme will make due with $\mathcal O(n^d)$ FLOPs.
There are, however ways to improve this. Seeing as the matrix $\mathbf A$ does not change as long as none of the parameters change we can use a LU-decomposition. 
This will demand a decomposition of $\mathcal{O}(n^3)$ FLOPs, but all the subsequent steps will be $\mathcal{O}(n^2)$ FLOPs ($\mathcal O(n^{2d})$ for higher dimensions). 
We are still not quite at the level of the explicit scheme, but it is a clear improvement. \\
Looking closer at $A$ we notice that it is not only sparse, but tridiagonal. This calls for further optimization which brings the required number of FLOPs down to $\mathcal O(n)$ making it equally efficient to the explicit scheme. More on tridiagonal Gaussian elimination later.


\subsection{Stability}\label{stability}

In section \ref{discretizing} we used the Forward Euler approximation to the time derivative. 
Unfortunately the resulting scheme is potentially unstable, as we shall now see. 
We start out by assuming that the solution $u(x,t)$ is on the form 
\begin{equation}\label{von_neumann_fourier_solution}
 u(x,t) = A^n\exp(ikp\Delta x)
\end{equation}
where $i^2=-1$ is the imaginary unit and $A^n$ is an amplification factor which, for the solution \ref{von_neumann_fourier_solution} ideally should be $\exp(-\pi^2t)$, but will be something else in the numerical case. 
We notice that we must have $\left|A\right|\leq 1$ if $u$ is to not blow up. 
Inserting \ref{von_neumann_fourier_solution} in the simplified version of the variable coefficient scheme (where the coefficient is constant) gives us the following
\begin{align*}
 \exp(ikp\Delta x)\left(A^{n+1}-A^n\right) &= \\
 A^n\frac{D\Delta t}{\Delta x^2}&\left(\exp(ik(p+1)\Delta x)-2\exp(ikp\Delta x) +\exp(ik(p-1)\Delta x)\right)\\
  A^n\exp(ikp\Delta x)\left(A-1\right) &= \\
  A^n\exp(ikp\Delta x)\frac{D\Delta t}{\Delta x^2}&\left(\exp(ik\Delta x) -2  + \exp(-ik\Delta x)\right)
\end{align*}
Using the well known identities $\exp(iax)+\exp(-iax) = \frac{1}{2}\cos^2\left(\frac{ax}{2}\right)$  and $\cos^2(ax)-1 = \sin^2(ax)$ gives us
\begin{equation}
 A-1 = \frac{D\Delta t}{\Delta x^2}\sin^2\left(\frac{k\Delta x}{2}\right)
\end{equation}
We now insert for the ``worst case scenario'' $max(\sin^2\left(\frac{k\Delta x}{2}\right))= 1$
\begin{equation}
 A = \frac{D\Delta t}{2\Delta x^2}+1 \implies \Delta t\leq\frac{\Delta x^2}{2D}
\end{equation}
In 2d this criterion is halved, and for the anisotropic case we must insert for the maximum value of D which, again, will be the ``worst case scenario''.\\
If we insert the same solution (eq. \ref{von_neumann_fourier_solution}) in the BE scheme we get
\begin{align*}
 \exp(ikp\Delta x)\left(A^{n}-A^{n-1}\right) &= \\
 A^n\frac{D\Delta t}{\Delta x^2}&\left(\exp(ik(p+1)\Delta x)-2\exp(ikp\Delta x) +\exp(ik(p-1)\Delta x)\right)\\
  A^n\exp(ikp\Delta x)\left(1-A^{-1}\right) &=  A^n\exp(ikp\Delta x)\frac{D\Delta t}{\Delta x^2}\left(\exp(ik\Delta x) -2 + \exp(-ik\Delta x)\right)\\
\end{align*}
which leads to 
\begin{equation}\label{stability_BE}
A = \frac{1}{ 1+\frac{D\Delta t}{\Delta x^2}}
\end{equation}
Equation \ref{stability_BE} is smaller than 1 for all $\Delta t>0$ which means that the scheme is unconditionally stable.

\subsection{Truncation error}\label{truncation_error}

As we know the numerical derivative is not the analytical derivative, but an approximation. 
This approximation has a well defined residual, or truncation error which we can find by Taylor expansion.
\begin{equation*}
  R = \frac{u(t_{n+1}) -u(t_n)}{\Delta t} -u'(t_n)
\end{equation*}
Remember Taylor expansion of $u(t+h) = \sum\limits_{i=0}^\infty\frac{1}{i!}\frac{d^i}{dt^i}u(t)h^i$
\begin{align*}
 R &= \frac{u(t_n)+u'(t_n)\Delta t +0.5u''(t_n)\Delta t^2 + \mathcal{O}(\Delta t^3)-u(t_n)}{\Delta t} -u'(t_n)\\
  &= u''(t_n)\Delta t+ \mathcal{O}(\Delta t^2) = \mathcal{O}(\Delta t)
\end{align*}
We can do better than this by using another discretization scheme for the PDE, but in our case the PDE is not the only error source seeing as we will combine it with a random walk solver. 
Quantifying an error term The block-tridiagonal solver is taken from \cite{}, and is listed below.\\for the random walk solver is not straightforward, but naturally it will be closely coupled to the number of walkers used. 
So far the error seems to behave as expected, meaning that introducing very many walkers might reduce the error to $\mathcal{O}(\Delta t^2)$ if the number of walkers, $N$ is proportionate to $N\propto\frac{1}{\Delta t^2}$. 
Since $\Delta t \leq\frac{D\Delta x^2}{2}$ by the stability constraint (in 1D), we will already for small meshes of some 20 points need to introduce $\sim600000$ walkers per unit ``concentration'' per mesh-point in the walk-area. 
This will be such a costly operation that it will not necessarily be worth it.\\
The spatial derivative also has a well defined residual which is found by Taylor expansion.
\begin{equation}
R = \frac{u(x_{i+1})-2u(x_i)+u(x_{i-1})}{\Delta x^2}-u''(x_i) 
\end{equation}
Remember Taylor expansion of $u(x-h) = \sum\limits_{i=0}^\infty\frac{1}{i!}\frac{d^i}{dt^i}u(x)(-h)^i$
\begin{align*}
 &R =\\& \frac{u(x_i)+u'(x_i)\Delta x +0.5u''(x_i)\Delta x^2 + \frac{1}{6}u^{(3)}(x_i)\Delta x^3) +\frac{1}{24}u^{(4)}(x_i)\Delta x^4 +\mathcal{O}(\Delta x^5)}{\Delta x^2}-\\ &\frac{2u(x_i)}{\Delta x^2}+\\
 &\frac{u(x_i)-u'(x_i)\Delta x +0.5u''(x_i)\Delta x^2 - \frac{1}{6}u^{(3)}(x_i)\Delta x^3) +\frac{1}{24}u^{(4)}(x_i)\Delta x^4 +\mathcal{O}(\Delta x^5)}{\Delta x^2} \\ &-u'(x_i)\\
&R = u''(x_i) +\frac{1}{12}u^{(4)}(x_i)\Delta x^2 + \mathcal{O}(\Delta x^5)  -u''(x_i) = \mathcal{O}(\Delta x^2) 
\end{align*}
There are discretizations that can reduce this residual even further (although a second order scheme is usually considered adequate), but this time the stability criterion on the time derivative \ref{stability} will always be of the order $\mathcal{O}(\Delta x^2)$ and so we will never get a smaller error than this unless we change the time derivative. \\
For the Random walk we know from statistical mechanics that the fluctuations around a steady state is related to the number of walkers through eq. \ref{RW_error_estimate}.
\begin{equation}\label{RW_error_estimate}
 \langle\Delta u\rangle \propto \frac{1}{\sqrt{N}}
\end{equation}
In the combined solver, we assume that equation \ref{RW_error_estimate} still holds for the RW-part of the solution even though we can only say for certain that it is correct for the first time-step. 
The number of walkers, $N$ is now given by the defined conversion factor $Hc$ as 
\begin{equation}
 N(x,y,t) = Hc\cdot U(x,y,t)
\end{equation}
and the total number of walkers is the sum of the walkers on all the mesh-points. 
In each mesh-point the fluctuations are of the order $\sqrt{N(x_i,y_j,t_n)}^{-1}$, meaning that the convergence rate in each mesh-point is $\frac{1}{2}$. 

% \subsection{Extension to 3 spatial dimensions}
% 
% As we see in the appendix the assembly of the linear problem that arises from the BE discretization in 2d with a variable diffusion constant is a rather messy thing. 
% While a 2d simulation might tell us a great deal, and be sufficient for many applications such as modeling of experimental setups of diffusion in the ECS (in vitro experiments on very thin slices), we should at least look into an extension of our model to 3 dimensions. 
% The BE discretization is very similar in 3d to the 2d case. In fact we must only add a term for the z-direction. 
% If we translate the resulting expression to a linear problem we immediately come across the problem that our solution, U, is a cubic matrix and so we must have a 4-dimensional matrix to describe to system. 
% The problem is solved by first of all describing the solution as a vector of matrices, and in turn as a vector of vectors (which is a matrix). 
% We now have the same situation as we had in the 2d case, and by following the same procedure as before we arrive at exactly the same situation which is a normal linear problem. 
% In fact, using block-matrix notation we can even write the problem in an identical way, as we have done in the appendix (eq. \ref{BE3D_linear_problem}). 
% The matrix is now 7-band diagonal, but the LU-decomposition still does the trick.\\
% Just as for the 2d case, the computiational cost per time step is of the order of $\mathcal{O}(N^2)$ where $N$ is now the spatial resolution cubed ($N = n^3$). 
% In other words the computiational cost is now quite high ($\mathcal{O}(n^6)$).
% For the actual decomposition the cost is $\mathcal{O}(N^3) = \mathcal{O}(n^9)$

\subsection{Tridiagonal linear systems}
The implicit discretization gives us a set of linear equations, or a linear system, to solve at each timestep. 
The physics of the system gives us a special form of linear system, namely a band diagonal system, where the number of non-zero bands on the matrix is dependent on the number of spatial dimensions we are in. 
The one dimensional case gives us a tridiagonal system, which can be solved extremely efficiently by the ``tridiag'' function listed above.
In two spatial dimensions we are not quite as fortunate as in one dimension. We get a banded matrix with 2n bands and five non-zero bands, where n is the spatial resolution (which is equal in x and y direction). 
Rewriting the assembled matrix (see eq. \ref{linear_system_BE2D}) to a block-matrix form gives us a tridiagonal matrix, where the entries are $n\times n$ matrices. 


The fool-proof way to solve a linear equation $\mathbf{A}\mathbf{x} = \mathbf{b}$ where $\mathbf{A}$ is not a sparse matrix, by Gaussian elimination.

\begin{align}
  \mathbf{A} = 
 \left( \begin{array}{rrrr}
 a_{11} & a_{12} & a_{13} & a_{14} \\
 a_{21} & a_{22} & a_{23} & a_{24} \\
 a_{31} & a_{32} & a_{33} & a_{34} \\
 a_{41} & a_{42} & a_{43} & a_{44}
 \end{array} \right)\mathbf{x} = \mathbf{b}
\end{align}
Is reduced to
\begin{align}
  \mathbf{A} = 
 \left( \begin{array}{rrrr}
 a_{11} & a_{12} & a_{13} & a_{14} \\
 0 & (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) & (a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}}) \\
 0 & (a_{32}-\frac{a_{31}a_{12}}{a_{11}}) & (a_{33}-\frac{a_{31}a_{13}}{a_{11}}) & (a_{34}-\frac{a_{31}a_{14}}{a_{11}}) \\
 0 & (a_{42}-\frac{a_{41}a_{12}}{a_{11}}) & (a_{43}-\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\frac{a_{41}a_{14}}{a_{11}})
 \end{array} \right)\mathbf{x} = \tilde{\mathbf{b}}
\end{align}
and further to
\begin{align}
  \mathbf{A} = 
 \left( \begin{array}{rrrr}
 a_{11} & a_{12} & a_{13} & a_{14} \\
 0 & (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) & (a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}}) \\
 0 & 0 & (\tilde{a}_{33}-\frac{\tilde{a}_{32}\tilde{a}_{23}}{\tilde{a}_{22}}) & (\tilde{a}_{34}-\frac{\tilde{a}_{32}\tilde{a}_{24}}{\tilde{a}_{22}}) \\
 0 & 0 & (\tilde{a}_{43}-\frac{\tilde{a}_{42}\tilde{a}_{23}}{\tilde{a}_{22}}) & (\tilde{a}_{44}-\frac{\tilde{a}_{42}\tilde{a}_{34}}{\tilde{a}_{22}})
 \end{array} \right)\mathbf{x} = \tilde{\mathbf{b}}
\end{align}
until we have an upper triangular matrix. We then do a backwards sweep to solve for one element of the unknown vector, $\mathbf{x}$ at a time. 

Since most entries are zero we can easily get away with only doing one forward sweep down the matrix, eliminating all the sub-diagonal matrix-entries, and then one backward sweep, which calculates the unknown vector $\mathbf{x}$. The algorithm is listed below as a function implemented in C++.

\lstinputlisting{Figures/tridiag.cpp}
We can also modify the tridiagonal solver from the one-dimensional case so we can use it on block-tridiagonal systems. 
The modified algorithm for the block-tridiagonal matrix \ref{block_tridiagonal_matrix} is listed in \ref{}, and is in fact only the linear algebra version of the ``tridiag'' function, replacing the $1.0/btmp$ -terms with $\left(B_i+A_iH_{i-1}\right)^{-1}$. The result of rewriting the $2n$-band diagonal matrix is the block matrix in equation \ref{block_tridiagonal_matrix}.

\begin{align}\label{block_tridiagonal_matrix}
   \left(\begin{array}{c c c c c c c c c}
        B_0 & C_0 &0 &0 &0 &0 &0 &0 &0\\
        A_1 & B_1 & C_1 &0 &0 &0 &0 &0 &0\\
        0&\ddots & \ddots & 0 & 0 & \ddots &0&0&0\\
        0 & 0&A_i & B_i & C_i& 0 &  &0&0\\
        0& \ddots&0&\ddots & \ddots & \ddots & 0 & \ddots &0\\
         0&0 &0 &0&0 &0&0&A_{n-1} & B_{n-1}
       \end{array}\right) \left(\begin{array}{c}
             \mathbf{u}^{n+1}_{0}\\
             \mathbf{u}^{n+1}_{1}\\
             \vdots\\
             \mathbf{u}^{n+1}_{i}\\
             \vdots\\
             \mathbf{u}^{n+1}_{n}
             \end{array}\right) = \mathbf{u}^{n}
\end{align}
which we can also express as $M\mathbf{x} = \mathbf{k}$. Block-matrices named $B_i$ are tridiagonal, and the ones named $A_i$ or $C_i$ are strictly diagonal. 

There is a forward substitution
\begin{align*}
 H_1 &= -B_1^{-1}C_1\nonumber \\
 H_i &= -\left(B_i+A_iH_{i-1}\right)^{-1}C_i \nonumber \\
 \mathbf{g}_1 &= B_1^{-1}\mathbf{k}_1 \nonumber\\
 \mathbf{g}_1 &= \left(B_i+A_iH_{i-1}\right)^{-1}\left(\mathbf{k}_i-A_i\mathbf{g}_{i-1}\right)\nonumber
 \end{align*}
 Followed by a backward substitution
 \begin{align*}
  \mathbf{x}_{n-1} &= \mathbf{g}_{n-1}\nonumber\\
  \mathbf{x}_i &= \mathbf{g}_i + H_i\mathbf{x}_{i+1} \nonumber
 \end{align*}
The algorithm requires inverting approximately 3n $n\times n$ matrices, which might be expensive. 
However, we only need to to the inversion once as long as the mass-matrix, $M$ is unchanged so the expense is reduced. 
This should give us a computational intensity of around $\mathcal{O}(n^2)$ seeing as we only need to do one matrix-matrix multiplication where one matrix is diagonal, and two matrix-vector multiplications. 
All of which demand $\mathcal{O}(n^2)$ operations. This reduction in computational cost makes the implicit scheme as effective as the implicit FE scheme.

In three dimensions we are even more unfortunate and get an $2n^2$-banded matrix and seven non-zero bands. 
As we can see in the appendix (eq. \ref{BE3D_linear_system}) we can write this so it looks like the block tridiagonal linear system we got in 2d, and that could be solved by the block-tridiagonal solver. 
The difference is that the entries i n the block-matrices $A_i$, $B_i$ and $C_i$ are block-matrices themselves meaning that we must work with $n^2\times n^2$ matrices. 
All in all we should get a performance of around $\mathcal{O}(n^3)$ if we take care to save the inverted matrices from the forward substitution. 
This is about the same performance as the explicit scheme gives us, but again without the stability issue.

\section{Combining the two solvers}\label{combining_the_solvers}
This section will deal with the actual combination of the two models.\\

\subsection{The basic algorithm}\label{basic_algorithm}

The basic structure of the program is rather similar to the physical problem.
There is one dendrite-object which contains the PDE-solver for the normal diffusion equation, with the possibility to use a random walk solver instead. 
On the dendrite object spines can be placed, which in the physical world are the receiving end of a synapse. 
Depending on what is being modeled, synaptic input is modeled by randomly added spikes of some random number of molecules which spawn at the far end of the spine. 
In the overlapping points where the spine is located on the dendrite mesh, the coupling is done as follows: 
If a random walker in the spine comes in contact with the position labeled as the ``end'' of the spine it is moved from the list of active walkers to a list of walkers which have moved out of the spine. 
Similarly, at each time-step a part of the PDE-solution corresponding to one walker will diffuse into the spine with a certain probability. 
It might be desirable for a walker to only be able to diffuse out of the spine with some probability as well, or for the walkers which diffuse into the spine to have some drift term, but these are minor updates and might be added later if needed. \\

There is also the possibility of modeling parts of the dendrite-mesh as random walk (This can be done in 2d as well as 1d). 
This is done by choosing some points on the mesh and sending the to the ``AddWalkArea'' method which will map them to an index and set the initial condition for the walk. 
Although anisotropy will follow into the random walk solver, by the method provided by Farnell and Gibson \cite{farnell2005monte}. 
At each time-step the solve-method of the combined solver is called, which in turn calls the solve method for the PDE-solver. 
The solution from the PDE-solver is used to calculate the number of walkers by eq. \ref{} in each mesh-point on the PDE-mesh, and then give each walker a random position in a square around its mesh-point ($\pm \frac{\Delta x}{2}$). 
Because the sum of the PDE-solution over the random walk area of the mesh might be different from one time-step to the next (eq. \ref{integral_u}) the conversion from PDE-solution to random walker distribution must be done at every time-step. 
The alternatives are to remove or add the difference at each time-step, but this will require checking that each mesh-point has the ``correct'' number of walkers and updating the number to correspond with the solution from the PDE-solver. \emph{which is what we are doing already?}
Or the conversion factor could be adjusted at each time-step. The latter is largely a bad solution because it ruins transparency and might introduce even more fluctuations in the solution.

\begin{equation}\label{integral_u}
 \sum u_{i,j}^n \neq \sum u_{i,j}^{n+1}
\end{equation}
After the random walk integration the two solutions are combined by a simple average. A few other methods have been tested (see chapter \ref{}) but discarded. 
The average of the two solutions is then set as the new ``initial condition'' for the next time-step, and the  process repeats itself.

The results of these are inserted in the solution from the PDE using some routine (e.g. the average of the two) and the time-step is done. 
A schematic of the algorithm is provided in figure \ref{schematic}.

\begin{figure}[H]
\centering
% \includegraphics{Figures/schematic.eps}
\caption[Algorithm]{Schematic diagram of the algorithm.}
\label{schematic}
\end{figure}

\subsection{Convergence rate}

In chapter \ref{truncation_error} we discussed the error that arises as a result of adding random walkers on parts of the mesh. 
We found that the amplitude of the fluctuations per mesh-point is proportionate to $\frac{1}{\sqrt{N}}$ where N is the number of walkers related to the mesh-point. 
Combining the two models means adding fluctuations to the approximation to the exact solution. Seeing as we send this combined solution to the PDE-solver as an ``initial condition'' for the next time-step we have made a compromise in accuracy. The error-estimate we will define later is still dependent of $\Delta t$, but the dependency is now of the order $\mathcal{O}(\sqrt{\Delta t})$. 
This also further supports the claim that there is no need to find a very precise scheme to solve the PDE.\\
\emph{We will test this by doing a convergence test in time keeping the number of walkers constant.}

\subsection{Potential problems or pitfalls with combining solutions}\label{problems_and_pitfalls}
 
In this chapter we will identify and discuss a few obvious difficulties we can expect to run into in our planned project. As far as possible we will also explain how to solve the problems or how to live with them.

\begin{itemize}
 \item Different timescales\\
  The PDE-solver will be operating with some time-step $\Delta t$ which will, depending on the discretization of the PDE, have some constraints and will definitely have an impact on the error. 
  The walkers will, as we have just seen, solve the diffusion equation as well, but with some different $\Delta \tilde{t}$ which is smaller than the time-step on the PDE level. 
  Depending on the coupling chosen between the two models this difference will have some effect or a catastrophic effect on the error. 
  Running some number of steps, N, on the random-walk level should eventually sum up to the time-step on the PDE level, $\sum\limits_{i=0}^N \Delta\tilde{t} = \Delta t$. 
  It turns out, as we will see in section \ref{probability_distribution_and_timesteps} that we can make sure the coupling is as good as it gets by restricting the step length of the walkers.
 \item Boundary conditions\\
 To combine the two models we will need to put restricting boundary conditions on the random walks. This is not usually done (as far as I have seen), but not very difficult. 
 Finding a boundary condition that accurately models the actual system turns out to be quite straightforward.
 We can assume that the number of walkers in the walk-domain is conserved for each PDE time-step, and thus no walkers can escape the domain. 
 Implementing perfectly reflecting boundaries solves this quite well. 
 This means that the flux of walkers out of a boundary is zero, which is the same as Neumann boundary conditions on the PDE level. \\
 Dirichlet boundaries can (probably) be implemented by adding or removing walkers on the boundaries (or in a buffer-zone around them) until we have the desired concentration of walkers.
 \item Negative concentration of walkers \\
 The concentration of walkers is calculated as $NP(x,t)$ where $P(x,t)$ is really only an estimate of the actual probability distribution, calculated by dividing the number of walkers in one area $x\pm\frac{\Delta x}{2}$ by the total number of walkers. 
 Seeing as negative probabilities does not make sense, and neither does a negative number of walkers, we will eventually run into some problems if the solution of the PDE takes negative values (which it most likely will not do). 
 I can find no good solutions to this problem, but a workaround consists of storing the sign of the solution over each time-step, converting the absolute value to a distribution of random walkers and multiplying back with the sign after the RW solution is done. 
 This workaround has a problem in that a transition from positive to negative value will lead to a ``valley'' in the absolute-value solution. 
 A normal PDE solution of this kind of initial condition will very rapidly even out the ``valley'', and so a value which should have been zero (a node-point) will get some other value (say some fraction of the conversion factor). This again leads to a larger discontinuity when the solution from the RW model is multiplied by the sign again.
 \item Smooth solutions\\
 A diffusion process is very effective when it comes to dampening fast fluctuations, and so any solution of the diffusion equation will be smooth. 
 When we introduce a stochastic process, we will potentially also introduce fast fluctuations from one time-step to the next. 
 In this case we are faced with a dilemma; on the one hand there is the smoothness of the solution to consider, on the other hand we have introduced the stochastic term believing that it adds detail to our model. 
 The approach we tried to use to this was to do some curve-fitting using both of the solutions. 
 A polynomial regression model was implemented in 1d, but regardless of degree and what points were used, the result was a lot worse than just the average of the two solutions. 
 Another idea is to implement a cubic spline interpolation over the area, but this too has its problems. An interpolation forces the solution to have a value at the interpolating points, and seeing as we cannot say for certain which value is correct how shall we pick the interpolating points?
 \item Number of time-steps on the random walk level\\
 As the time-step on the PDE level is increased above the stability criterion of the FE scheme towards more efficient sizes we are faced with the problem of whether or not to increase the number of time-steps on the RW level. 
 Strictly speaking we do not have to do this, seeing as we adjust the step-length of the walkers with respect to the time-step (see eq \ref{steplength}). 
 As an initial value we put the number of time-steps to 100, but this was more a guess of how many are necessary for the central limit theorem to have effect than anything else. 
 The question really boils down to how we define our model, which we have yet to do in an accurate way.
 \item Random walks in 3D\\
 Both 1 and 2 dimensional space are spanned completely by a random walk, but space of 3 or more dimensions is not. 
 This does not have to be a problem, seeing as we have proved that the random walk fulfills the diffusion equation (chapter \ref{more_general_random_walks}) and we are not trying to span the complete 3d space, but we could potentially meet some difficulties as a result of this property of the random walk.
\end{itemize}

\subsection{Probability distribution and time-steps}\label{probability_distribution_and_timesteps}
As we saw in section \ref{more_general_random_walks} the probability of finding a walker at a position $x_i$ after some $N$ time-steps (on the walk-scale) is (in the limit of large $N$) given as the Gaussian distribution. 
In our application, however, we are not interested in finding the walker at an exact position, but in an interval around the mesh-points sent to the walk-solver. 
This interval is (for obvious reasons) $x_i\pm\frac{\Delta x}{2}$ where $\Delta x$ is the mesh resolution on the PDE level. 
We will also run the walk solver for some $N$ time-steps on the random-walk scale (where $N$ steps on the random walk scale is the same as one step on the PDE scale). 
This slightly modifies our distribution into
\begin{equation}
 P(x_i\pm\Delta x,t_{n+1}) = \frac{1}{\sqrt{4\pi DN\Delta \tilde{t}}}\exp\left(\frac{(x\pm\Delta x)^2}{4DN\Delta \tilde{t}}\right)
\end{equation}
This makes the concentration of walkers $C(x,t) = MP(x,t)$
\begin{equation}
 C(x_i\pm\Delta x,t_{n+1}) = \frac{M}{\sqrt{4\pi DN\Delta \tilde{t}}}\exp\left(\frac{(x\pm\Delta x)^2}{4DN\Delta \tilde{t}}\right)
\end{equation}
For each PDE time-step we reset the walkers to have some new initial condition. 
We do this because the concentration over the ``walk-area'' will change with each PDE time-step.
The point is that $ C(x_i\pm\Delta x,t_{n+1})$ will be dependent on the initial condition $ C(x_i\pm\Delta x,t_{n})$.


Looking at the difference in time-step size between the two length scales we see from equation \ref{descrete_gaussian_distr} that the step-size on the random walk scale is dependent on the variance in the actual steps (This is in principle the Einstein relation). 
\begin{equation}
 \sigma^2 = \langle\Delta x^2\rangle = 2DN\Delta\tilde{t} \implies \Delta\tilde{t} = \frac{\langle\Delta x^2\rangle}{2DN}
\end{equation}
Equating this with \ref{random_walk_variance} gives us a first order approximation to the step-length, $l$
\begin{align}
 \langle\Delta x^2\rangle &= 4pqNl^2 = 2DN\Delta\tilde{t} \nonumber \\ 
 l &= \sqrt{2D\Delta\tilde{t}}. \label{steplength}
\end{align}
Of course this is assuming that we use a random walk algorithm of fixed step-length.\\
Equation \ref{steplength} is proportional to the square-root of the adjusted time-step. 
We have already suggested that the error term from the RW simulation depends on the number of walkers we use (or the conversion rate). This equation suggests that the error term also depends on the time step. 
Though this might seem a bit frustrating at first glance, but it answers a question we asked earlier; how many time-steps do we need to take at the RW-level. 
We have the intuitive relation between the RW time-step, $\Delta \tilde t$ and the PDE time-step through the number of steps at the RW level, $T$:
\begin{equation*}
 \Delta \tilde t = \frac{\Delta t}{T}
\end{equation*}
Further we suggested that the error from the RW simulation is proportional to the root of the time-step $\epsilon \propto \sqrt{\Delta \tilde t} = \sqrt{\frac{\Delta t}{T}}$. We want to force this error to behave as $\mathcal O(\Delta t)$, and see that we can adjust the number of time-steps at the RW level in order to make this happen.
\begin{equation}
 \mathcal O(\Delta t)>\sqrt \frac{\Delta t}{T} \implies T>\frac{1}{\Delta t}
\end{equation}

Combined with the demand to the number of walkers we will quickly end up with an extreme computational demand if we want to force our model to have first order convergence (not to mention second order convergence). 
Fortunately we will ignore this demand outside of verification because there is little physical meaning left if we use the demanded number of walkers.

\section{Geometry}
Any finite difference method is problematic to solve on anything else than a rectangular grid. 
When we additionally use an implicit FD method we will add a ``demand'' of having a square grid as well. 
Fortunately the implicit solvers we use are stable. \\
In the physical scope of this thesis we do not find any rectangular shapes to apply our system to. 
Actually we do not have any well defined geometry to apply our system on, but that is of less importance. 
The purpose of this project is to investigate the actual coupling of two models for the same problem. \\
If we want to model diffusion on a general geometry by a FD method we could transform the grid to a unit-square through a general transform.
\begin{equation*}
 \vec{r} = \mathbf{T}(\vec{q})
\end{equation*}
Where $\vec{r} = (x,y)$ is the position in physical space, $\vec{q} = (\xi,\eta)$ is the position on the unit-square that is the computational space and $\mathbf{T}$  is the transformation. The transformation is achieved by the functions $x(\xi,\eta)$ and $y(\xi,\eta)$. After a lot of math, including some differential geometry we find that the diffusion equation in computational space still can be written as
\begin{equation*}
 \frac{\d C}{\d t} -\nabla\cdot\vec{J} = 0
\end{equation*}
but the total flux vector $\vec{J} = \vec{f} + \phi$ now has the properties
\begin{align*}
 \nabla\cdot\vec{f} = \frac{1}{g}\cdot\left(\vec{f}\cdot\frac{\d}{\d\xi}\left(\frac{\d y}{\d\eta},\frac{-\d x}{\d \eta}\right) + \vec{f}\cdot\frac{\d}{\d\eta}\left(\frac{-\d y}{\d\xi},\frac{\d x}{\d \xi}\right)\right) \\
 \nabla\phi = \frac{1}{g}\left(\phi\cdot\frac{\d}{\d\xi}\left(\frac{\d y}{\d\eta},\frac{-\d x}{\d\eta}\right)+\phi\cdot\frac{\d}{\d\eta}\left(\frac{-\d y}{\d\xi},\frac{\d x}{\d\xi}\right)\right)
\end{align*}
where $g$ is the Jacobian of the transformation $\mathbf{T}$. In other words we would need to discretize a completely new and much more complicated equation in order to solve for a general geometry. We would also need to have some idea of what the functions $x(\xi,\eta)$ and $y(\xi,\eta)$ are, which is not necessarily something we have access to. 
Furthermore we already have access to Finite Element software which can take any geometry as a mesh, and so a simpler way of using a more interesting geometry would be to change the PDE-solver in the developed software to a FEniCS solver.

