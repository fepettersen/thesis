\section{About}
The current version of the software (14.02.14) works in the following way. 
Various parameters are specified in a python-script which calls the program. 
After initialization, where anisotropic diffusion constant, initial condition and areas of combined solution are set up we start solving the equation (using the implicit BE scheme). 
At each time-step we call the Solve-method of the combine-class which in turn calls the solve-method of the PDE-solver, calculates the number of walkers by the conversion-rate, gives them a random position in proximity to the coarse-grained PDE-mesh-point we are looking at, and writes this position to a file. 
The random walk program is then called with some parameters. This program reads the file with walker-positions, advances some specified number of time-steps and writes the final positions to the same file it read. 
The main program now reads this file, maps the walker-positions back to the coarse-grained PDE-mesh and converts the spatial distribution of walkers to a concentration distribution. 
Here we are faced with a few choices with respect to the combination of the two solutions we have for the same area: 
\begin{itemize}
 \item Some form of least squares fitting could be done. A polynomial regression was developed, but tests indicate that it is not a good solution.
 \item Cubic spline interpolation might be slightly better seeing as we force the derivatives to be equal at the end-points. 
 Interpolation forces the fitted curve to take the measured values at the interpolation-points, and so we must chose only some of the points to be used by the interpolation in order for there to be a difference. Immediately we are faced with the problem of which points to use and which to throw away. If we consistently chose the points which are closest to the PDE-solution (and the end-points which must be included for smoothness) we might as well not include a random walk model, and vice versa.
 \item We could use only the result from the random walk model
 \item Some sort of average might also help us. There are many to chose from, both arithmetic and geometric and with different kinds of weighting.
\end{itemize}
Intuitively, I find it better to use either an average or only the random walk result. At present (14.02.14) the simplest arithmetic mean is implemented at each mesh-point.\\
After each time-step the program also writes the combined solution on the coarse-grained mesh to a unique file.\\
The python script also does some other more or less clever stuff at each run. This is described in the appendix as a form of debugging \ref{debugging}.

\subsection{Limitations}

As with any software there are limitations. The limitations discussed in this chapter will be with regard to physical problems, and not memory or CPU limitations which are described later. \\
Assuming the dendritic spines stuff will be the final application:\\
First of all, to specify a source function, one must program it in the Diffusion class (Diffusion::f()). This might be fixed in a later version using inheritance.\\
The geometry is, currently, limited to a square. Of course we can argue that we have scaled the size to a unit square, but as discussed in chapter \ref{geometry} the actual geometry remains a unit square. 
Furthermore the same issue arises on the random walks. \\
As of now, it is unclear if 3D-modeling is supported. This is actually a smaller issue since there is usually little more to learn from switching from 2D to 3D. In any case, the extension to 3D is most likely not very hard, provided we use an array of slices from the cubic matrix describing the solution.

\section{Adaptivity}

There are two adaptive parts of the software. First of all, the number of walkers which depend on the concentration, or the solution the the PDE in the relevant area. This must change in order to keep physical meaning and give results. Without this adaptivity, the results would either be wrong, or the model would not make physical sense.\\
Since a diffusion process in general has rapid changes in the beginning where for example high frequency variations are dampened and very slow convergence to a steady state later, we have introduced a test of the amount of change between two subsequent time-steps. If this amount is smaller than some limit, we will increase the time-step.\\
\emph{This increase should be done in a more elegant manner(linearly?)}

\section{Computational cost}
This chapter will consider the expensive parts of the code and look at possible improvements.

\subsection{Memory}
The memory-expensive parts of the code include storing the decomposed matrices, and storing the random walkers. 
None of these pose any problems.

\subsection{CPU time}
The program as it stands now (v 1.$\infty$) uses the BE discretization and a highly specialized tridiagonal solver. 
The random walk-part of the software has been excreted as a stand-alone program which communicates with the main-program through a binary .xyz file containing the positions of all walkers in 3d space. This makes it easier to change solver, and implement more advanced solvers like the Direct Simulation Monte Carlo (DSMC) Molecular Dynamics (MD) simulator written by Anders Hafreager (see chapter \ref{}). 
There are three expensive operations in the algorithm as it stands now with the BE discretization using standard LU-decomposition.
\begin{itemize}
\item Random walks are expensive if there are many walkers. The number of calls to the random number generator follows eq. \eqref{calls_to_ran} and for the verification process, which required a lot of walkers, this represented a considerable cost. 
\begin{equation}\label{calls_to_ran}
 N_{\text{calls}} \propto Hc\frac{(x_1-x_0)}{\Delta x}\frac{(y_1-y_0)}{\Delta y}\tilde{T}
\end{equation}
where $\tilde{T}$ denotes the number of time steps on the PDE level times the number of time-steps one PDE-step corresponds to on the RW level. Note that this expression will NOT be zero in 1D, and that it is dependent on the PDE-solution.
As an additional problem we will encounter some overhead upon calling the program, initializing variables and instances and so on. 
The computationally most demanding function seems to be the round-off function, which is used to place the walkers on the coarse-grained PDE-mesh.
\item Communicating the positions of all the walkers between the two programs each time-step is very costly.
\item Translating the positions of the walkers from the unit-square they are walking on to the coarse-grained PDE grid requires calling the ``round'' function from the math library in C++. This function is rather slow, and the program suffers from it. 
\end{itemize}


\subsection{Parallelizability}
In the final algorithm there are the following stages
\begin{itemize}
 \item Initialization\\
 Read parameters from command-line, initial condition and diffusion ``tensor'' from file. Setup instances of solvers etc. Practically no point in parallelizing this.
 \item LU-decomposition\\
 The actual LU-decomposition is a sort of Gaussian Elimination which is costly ($\mathcal{O}(N^3)$). Although the decomposition is pre-implemented at this point, the plan is to implement my own version of this. This step should be possible to parallelize.
 \item Solving\\
 This step includes a back-transform of the decomposed matrix which is expensive for $d>1$. This step should be parallelizable. It also includes the random walk part which is both expensive (depending on the number of walkers left) and highly parallelizable. We also write stuff to file which is quite costly. This is probably not possible to parallelize.
\end{itemize}

Parallelization of the random walk solver should scale linearly because the only form of communication required is shared memory. 
The LU decomposition and back-substitution require some communication and will not scale linearly, but will still benefit from parallelizing. 

\subsection{Some fancy title about changing stuff}

It should be rather simple to replace parts of the code as long as certain conditions are met. Perhaps the simplest part to replace is the random walk part. 
Requirements for this part are:
\begin{itemize}
 \item Locating executable main-file of the program in the folder ``stochastic'' and naming it ``walk\_solver''. 
 \item This executable should read the filename of the ini-file containing the positions of all walkers , and the local diffusion ``tensor'' in the relevant area.
 \item Upon completion, all positions must be written to the same file.
\end{itemize}
The PDE-solver should also be rather simple to replace, but some more programming will be required. First of all your solver must be included in the header-file. 
Next, the ``Combine'' class has an instance of the PDE-solver which it calls the advance-method of at each time-step. Seeing as this method is most regularly named ``solve'' you will either have to rename the method or the call. 
There are really very few dependencies on the PDE-solver, seeing as it is mostly left alone, but in addition to being able to respond to function calls it must:
\begin{itemize}
 \item Have its own $\Delta t$ attribute named ``dt''.
 \item Work on ``double**'' data types for all spatial dimensions (or implement some form of workaround)
 \item Be able to respond to increasing the time-step. In practice this means that the solver should be implicit.
\end{itemize}
As discussed, the implementation of random walks on parts of the mesh will reduce the convergence-rate to 0.5, and so there are really only two reasons to implement a new PDE solver. 
The current one only implements Neumann boundaries, and consequently must either be modified or replaced in order to work with other boundary conditions.
It also only works on a square mesh. As discussed in section \ref{geometry} it will be immensely complicated to implement a grid transformation, and this is already implemented in most finite element software.

\section{Numerical model}

The numerical model of the PKC$\gamma$ diffusion problem is implemented as follows. 
In the same way as Craske et.al \cite{craske2005spines} a section of dendrite will be modeled by one-dimensional diffusion. 
This dendrite section is thought to be in contact with the cell soma, and to have some spines on it. 
The soma is modeled as a source at the one end of the dendrite section, whereas the other end of the dendrite section is thought to continue branching into narrower and narrower dendrite branches. 
The branching and the rest of the dendrite will not be modeled in the beginning at least, because the situation in thin branches is thought to be the same as in thick branches with a time-delay and some effects due to a higher surface to volume ratio, all of which are described by Craske et.al. 

Dendritic spines are modeled as a two-dimensional funnel attached to the dendrite at some number of mesh-point which must correspond to the typical range of spine-neck diameters. 
This limits the spatial resolution by demanding the narrowest spine necks to connect at two mesh-points (in order for the spine to be two-dimensional). 

The model can without very much effort be extended to model an entire neuron with a dendrite tree (some modification to the dendrite class is required in order to add a dendrite to an existing dendrite) by making a neuron class containing a dendrite tree linked list and possibly a soma object should one wish to model the soma individually. 
Other possible extensions include adding and removing spines according to the rules proposed by (\emph{artikkelen Gaute snakket om}) in order to investigate how this affects various diffusion processes in dendrites or dendrite trees. 
Recall that the possibility to add spikes in spines is already built-in.

\subsection{Parameters}

As is the general problem in computational neuroscience this project requires setting a lot of parameters whose values are not known. 
There are of course a lot of parameters with known values as well, like the diffusion coefficient for PKC$\gamma$. 
A summary of the parameters of particular interest is shown in table \ref{parameter_values}, with an indication of where the values are taken from, if they are calculated or if they are simply given a value to ensure reasonable behaviour.

\begin{table}[H]
\centering
\begin{tabular}{|p{0.13\textwidth}|p{0.21\textwidth}|p{0.18\textwidth}|p{0.37\textwidth}|}
\hline
\textbf{Parameter} & \textbf{Explanation} &\textbf{Expression/ typical value}& \textbf{Origin} \\
\hline
$\Delta t$ & time-step & $\Delta x^2$ & stability criterion (sect. \ref{stability}) \\
\hline
$\Delta x$ & spatial resolution & $\frac{1}{2}$min(spine neck diameter) & estimated \\
\hline
$Hc$ & conversion factor & 5-24 & estimated by calculations of concentration levels taken from \cite{light1996protein} and spine/dendrite volume ratios. See later for discussion. \\
\hline
$u(t=0)$ & initial condition value & 5$\frac{\text{nMol}}{\text{L}}$ & estimated from values found in \cite{light1996protein}\\
\hline
$p_{ds}$ & probability to diffuse into a spine & $0.1\cdot\Delta x\cdot\Delta t$& estimated. An important ability of this parameter should be that wide necked spines have larger probability and that a certain flux is maintained (on average), meaning that the flux should be independent of $\Delta t$ \\
\hline
$p_{ab}$ & probability for PKC$\gamma$ to be absorbed, and removed from simulation, per time-step taken in spine head & 100\% & estimated\\
\hline
\end{tabular}
\caption[Important parameters]{Parameters which play an important role in the simulation of PKC$\gamma$ diffusion into dendritic spines with explanations, expressions/typical values and an indication as to where the value/expression has its origin.}
\label{table:parameters}
\end{table}

The conversion factor $Hc$ which was defined in equation \eqref{Hc_definition} must also take reasonable values in the simulation, and this might mean changing it to another expression. 
Light et.al. \cite{light1996protein} state that the concentration of conventional PKC which PKC$\gamma$ is a part of, is typically 20$\frac{\text{nMol}}{\text{L}}$ in cardiac cells. 
There is reason to believe that these values are typical for all cells. 
Since there are four conventional PKC types ( $\alpha$, $\beta_{I}$, $\beta_{II}$ and $\gamma$), the assumption that PKC$\gamma$ makes up for a quarter of this concentration (5$\frac{\text{nMol}}{\text{L}}$) is made. 
The dendrite is already being modeled as a one-dimensional object, meaning that it is assumed to be much wider than a spine. 
Wide dendrites can have a diameter of around 10$\mu$m and around $50\mu$m long before they start branching \cite{wikipedia??}, making the volume some $3900\mu\text m^3$. 

The unit $\frac{\text{nMol}}{\text{L}}$ is converted to more manageable units below 
\begin{align*}
 1\frac{\text{nMol}}{\text{L}} &= 10^{-15}\frac{\text{nMol}}{\mu\text{m}^3}\\
 &= 10^{-24}\frac{\text{Mol}}{\mu\text{m}^3}\\
 &\simeq 0.6\frac{\text{particles}}{\mu\text{m}^3}
\end{align*}

Craske et.al. measured an increase of $5\frac{\text{nMol}}{\text{L}}$ in the spine heads they were investigating \cite{craske2005spines} which, using numbers from \cite{arellano2007ultrastructure} for average spine volumes, is roughly one PKC$\gamma$ particle. 
In the simulations, this means that the total available concentration at the contact-point between the dendrite and a spine must exceed one particle. 
Assuming that the dendrite is cylindrical gives a cylinder volume segment with height equal to the neck width of the spine in question in which a concentration equal to at lest one particle must be present before any diffusion into the spine can take place. 
This makes $Hc \in (5,24)$ since that is the way we have defined $Hc$. 
In other words, the integrated concentration must exceed some value defined either directly by the spine neck width or some average neck width depending on how much control we want to have over $Hc$.

The initial condition is open to discussion, but at the moment a skewed Gaussian function i used with a peak value between 1 and 5. 
Release of PKC$\gamma$ into dendrites is followed by binding of PKC$\gamma$ on the dendrite-wall, but in any case this resembles simply removing some portion from the simulation. 
This step can just as well be dropped, seeing as it does not provide any extra information, and so only some initial pulse is required. 
The Gaussian distribution is chosen for simplicity. 

All values in the simulations are scaled to reduce round-off errors. 
Positions are measured in $\mu$m, concentrations in nMol/L or as particle-numbers when in spines. Only the timescale has normal units of seconds. 
Craske et.al. did experiments on the scale of $<3$min, but most of the simulations go for even shorter times.