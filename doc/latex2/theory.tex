\section{Random walks}

A random walker is a point object which after a certain amount of time jumps a fixed step length either right or left with equal probability for jumping either way. 
In $d$ spatial dimensions the axis on which the jump happens is chosen at random before the direction of the jump is decided. \\
Say $N$ random walkers are placed at the same position within a volume, $V$ of arbitrary shape, to make a spatial distribution of walkers, $C = N\delta(x)$. 
Here, $\delta(x)$ is the Dirac Delta function \cite{boas2006mathematical}, defined by its properties
\begin{equation}
 \delta(x-a) = \begin{cases}
              0 ,\;\;\; x\neq a\\
              \infty,\;\; x=a
             \end{cases}
\end{equation}

\noindent $V$ is contained by a surface $S$ and the relation is shown in Figure \ref{theory:divergence_theorem}. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{Figures/divergence_theorem.png}
 \caption[]{}
 \label{theory:divergence_theorem}
\end{figure}

\noindent A flux of walkers is a vector size describing the number of walkers passing a surface element $dS$ per unit time, per unit area. \\

\noindent As the random walkers start to move, some of them will leave $V$, which results in a change in $C$. 
The number of walkers leaving $V$ is described by the change in $C$ integrated over the entire volume,
\begin{equation}
 \int_V \frac{\d C}{\d t}\,dV.
\end{equation}
The number of walkers leaving $V$ must be equal to the number of walkers crossing $S$ per unit time and area. Integrating the outward normal component of the flux of walkers, $\mathbf{J}$ over the entire surface results in the number of walkers leaving $V$,

\begin{equation}\label{theory:flux_integral}
 \int_S\vec{J}\cdot \vec{n}\,dS.
\end{equation}
Since the number of walkers is conserved, the change in $C$ per time must be equal to the number of walkers leaving $V$ plus any walkers added to $V$:
\begin{equation}\label{theory:diffusion_integral_form}
 \int_V \frac{\d C}{\d t}\,dV = \int_S\vec{J}\cdot \vec{n}\,dS + s(x,t),
\end{equation}

\noindent where $s(x,t)$ denotes a source term. 
Through Greens theorem, \eqref{theory:flux_integral} can be reformulated 

\begin{equation}\label{theory:integral_form}
 \int_V \frac{\d C}{\d t}\,dV = \int_V\nabla\cdot\vec{J}\,dV +s(x,t).
\end{equation}

\noindent The flux $\vec{J}$ can be expressed by Fick's first law as the diffusive flux 
\begin{equation}\label{theory:difusive_flux}
 \vec J = D\nabla C,
\end{equation}

\noindent where $D$ is the diffusion coefficient. Since $V$ was chosen as an arbitrary volume \eqref{theory:integral_form} is independent of the integration and the integrals can be dropped. 
The diffusive flux $\mathbf{J}$ \eqref{theory:difusive_flux} is also inserted to yield the diffusion equation

\begin{equation}\label{theory:diffusion_equation}
 \frac{\d C}{\d t} = D\nabla^2 C +s(x,t).
\end{equation}

\noindent Throughout this thesis $C$ will denote a spatial distribution of random walkers while $u$ will denote the solution to the diffusion equation, which is a concentration distribution. 
The difference between $u$ and $C$ is best illustrated in Figure \ref{theory:illustration_C}.

\begin{figure}[H]
 \centering
 \begin{subfigure}{0.48\textwidth}
  \includegraphics[width=\textwidth]{Figures/illustration_C_pt1.eps}
  \caption{}
 \end{subfigure}
 \begin{subfigure}{0.48\textwidth}
  \includegraphics[width=\textwidth]{Figures/illustration_C_pt2.eps}
  \caption{}
 \end{subfigure}
 \caption[Illustration of $C(x)$]{Illustration of the difference between $u(x,t)$ in (a) which is the solution to the diffusion equation and $C(x,t)$ in (b) which is the number of random walkers within an area of $\pm\frac{\Delta x}{2}$ around each mesh point.}
 \label{theory:illustration_C}
\end{figure}

\subsection{An error estimate for a diffusion RW solver}

In general a Monte Carlo method will fluctuate around the correct (steady state) solution. The fluctuations have an amplitude related to the number of samples, $N$ used by the relation
\begin{equation}\label{theory:error_estimate_RW}
 \epsilon \propto \frac{1}{\sqrt N}.
\end{equation}

\noindent Equation \eqref{theory:error_estimate_RW} will be used as an error estimate for the solution of diffusion processes by a random walk (RW\nomenclature{RW}{Random Walk}) model and is assumed to hold for all parts of the simulation, not only the steady state.

\subsection{Random number generator}

To produce a random walk one must have random numbers. 
The random numbers in this thesis are produced by a five seeded xor-shift algorithm copied from George Marsaglia \cite{marsaglia2003xorshift}. 
This generator is chosen because of its very large period ($\sim10^{48}$) compared to the computational cost.

\section{Backward Euler schemes in two or more spatial dimensions}\label{theory:section:BE2D}

The reader is assumed to know the basics of solving partial differential equations (PDEs\nomenclature{PDE}{Partial Differential Equation}) by finite difference methods. For the sake of clarity the two schemes used to solve the diffusion equation \eqref{theory:diffusion_equation} in this thesis are written out using the theta rule for finite differences: 
\begin{equation}\label{theory_theta_rule}
 \frac{u^{k+1}-u^k}{\Delta t} = \theta D\frac{\d^2 u^{k+1}}{\d x^2} + (1-\theta)D\frac{\d^2 u^{k}}{\d x^2}.
\end{equation}
\noindent Setting $\theta = 0$ gives the Forward Euler (FE\nomenclature{FE}{Forward Euler}) scheme which is explicit and conditionally stable with stability criterion 
\begin{equation}\label{theory:stability_FE}
 \Delta t \leq \frac{\Delta x^2}{2d\max\{D\}}.
\end{equation}

Setting $\theta = 1$ gives the  Backward Euler (BE\nomenclature{BE}{Backward Euler}) scheme which is unconditionally stable. \\
This section will present a simple, but elegant way of efficiently solving the diffusion equation by the implicit BE scheme in more than one spatial dimension. The reason for using the BE scheme is that we want to use a large time step and still have a large spatial resolution. 
% This section will concern the special case of how to solve the diffusion equation by the implicit BE scheme, especially in more than one spatial dimension.
First, let us look at the BE discretization of the diffusion equation in 1D: 

\begin{align}\label{theory:BE_scheme_1D}
 u^{k+1}_i = \frac{\Delta t}{2\Delta x^2}\left((D_{i+1}+D_{i})(u^{k+1}_{i+1}-u^{k+1}_{i})-(D_{i}+D_{i-1})(u^{k+1}_{i}-u^{k+1}_{i-1})\right) + u^k_i.
\end{align}
Here $u$ is the unknown function which solves the diffusion equation. 
Note that 
$$u^k_i = u(t_0+k\cdot\Delta t,x_0+i\cdot\Delta x),$$
and not $u$ to the $k$'th power. \\
Neumann boundary conditions will be used, these are described in \eqref{theory:Neumann_boundaries}.

\begin{equation}\label{theory:Neumann_boundaries}
 \frac{\d u}{\d n} = 0|_{\text{on boundary}}
\end{equation}

\noindent Solving the BE scheme by hand for a very small mesh consisting of $4$ mesh points will reveal the structure of the BE scheme.
\begin{align*}
 &u^{k+1}_0 =  \frac{\Delta t}{2\Delta x^2}\left(2(D_{0}+D_{1})(u^{k+1}_{1}-u^{k+1}_{0})\right) + u^k_0\\
 &u^{k+1}_1 = \frac{\Delta t}{2\Delta x^2}\left((D_{2}+D_{1})(u^{k+1}_{2}-u^{k+1}_{1})-(D_{1}+D_{0})(u^{k+1}_{1}-u^{k+1}_{0})\right) + u^k_1\\
 &u^{k+1}_2 = \frac{\Delta t}{2\Delta x^2}\left((D_{3}+D_{2})(u^{k+1}_{3}-u^{k+1}_{2})-(D_{2}+D_{1})(u^{k+1}_{2}-u^{k+1}_{1})\right) + u^k_2 \\
 &u^{k+1}_3 =  \frac{\Delta t}{2\Delta x^2}\left(2(D_{2}+D_{3})(u^{k+1}_{3}-u^{k+1}_{2})\right) + u^k_3
\end{align*}

\noindent Rearranging this and setting $\alpha = \frac{\Delta t}{2\Delta x^2}$ results in a normal system of linear equations:
\begin{align*}
 &u^{k+1}_0\left(1+2\alpha(D_0+D_1)\right)- 2\alpha u^{k+1}_{1}(D_1+D_0) =  u^k_0\\
 &u^{k+1}_1\left(1+\alpha(D_2+2D_1+D_0)\right)-\alpha u^{k+1}_{2}(D_2+D_1)-\alpha u^{k+1}_{0}(D_1+D_0) = u^k_1\\
 &u^{k+1}_2\left(1+\alpha(D_3+2D_2+D_1)\right)-\alpha u^{k+1}_{3}(D_3+D_2)-\alpha u^{k+1}_{1}(D_2+D_1) = u^k_2\\
 &u^{k+1}_3\left(1+2\alpha(D_3+D_2)\right)- 2\alpha u^{k+1}_{2}(D_3+D_2) =  u^k_3
\end{align*}
which is arranged as a coefficient matrix multiplied by the unknown next time step

\begin{align}\label{BE}
 \left(\begin{array}{c c c c}
        b_0 & c_0 &0 &0 \\
        a_1 & b_1 & c_1 &0 \\
        0& a_2 & b_2& c_2\\
        0& 0& a_3 & b_3\\
       \end{array}\right)\mathbf{u}^{k+1} = \mathbf{u}^{k}
\end{align}
\noindent Denote the coefficient matrix in \eqref{BE} by $\mathbf M$
\begin{equation}\label{theory:BE:linear_system}
  \mathbf{M}\mathbf{u}^{k+1} = \mathbf{u}^{k}.
\end{equation}

\noindent If a linear system like the one in \eqref{theory:BE:linear_system} has a solution it can always be found by Gaussian elimination. 
However, for a sparse linear system a lot of the calculations in a Gaussian elimination will be done with zeros. 
A dramatically more efficient way to solve the linear system in \eqref{theory:BE:linear_system} is by exploiting the fact that it is tridiagonal. 
In order to be more consistent with the algorithm to be used, \eqref{theory:BE:linear_system} will be rewritten as
\begin{equation}
  \mathbf{M}\mathbf{u} = \mathbf{u}_p,
\end{equation}
where $\mathbf{u}_p = \mathbf{u}^{k}$ denotes the solution at the previous time step, and $\mathbf{u} =\mathbf{u}^{k+1}$ denotes the solution at the current time step. 
Tridiagonal systems can be solved extremely efficiently by the ``tridiag`` algorithm listed below.

{
\lstinputlisting[label=theory:tridiag,caption=The tridiag algoritm,language=c++]{Figures/tridiag.cpp}
 }


In two spatial dimensions the BE discretization gives the scheme shown below,

\begin{equation}\label{general_scheme_BE2D}
 u^{k}_{i,j} = -\underbrace{\frac{D\Delta t}{\Delta x^2}}_{\alpha}\left(u^{k+1}_{i+1,j}+u^{k+1}_{i-1,j}\right) +
 \underbrace{\left(1+\frac{2D\Delta t}{\Delta x^2} +\frac{2D\Delta t}{\Delta y^2}\right)}_{\gamma}u^{k+1}_{i,j} 
 -\underbrace{\frac{2D\Delta t}{\Delta y^2}}_{\beta}\left(u^{k+1}_{i,j+1}+u^{k+1}_{i,j-1}\right).
\end{equation}

\noindent If \eqref{general_scheme_BE2D} is solved by hand on a $3\times3$ mesh by the same steps as in 1D it gives the following linear system
% \noindent Equation \eqref{general_scheme_BE2D} assembles to the linear system in equation \eqref{linear_system_BE2D} when solved on a $3\times3$ mesh.
\begin{align}\label{linear_system_BE2D}
  \left(\begin{array}{c c c : c c c : c c c}
        \gamma & -2\beta &0 &-2\alpha &0 &0 &0 &0 &0\\
        -\beta & \gamma & -\beta &0 &-2\alpha &0 &0 &0 &0\\
        0&-2\beta & \gamma & 0 & 0 & -2\alpha &0&0&0\\ \hdashline
        -\alpha& 0&0 & \gamma & -2\beta & 0 & -\alpha &0&0\\
        0& -\alpha&0&-\beta & \gamma & -\beta & 0 & -\alpha &0\\
        0& 0& -\alpha&0&-2\beta & \gamma & 0 & 0 &-\alpha\\ \hdashline
        0& 0 &0 &-2\alpha &0&0 & \gamma & -2\beta&0\\
        0& 0 &0 &0 &-2\alpha&0&-\beta & \gamma &-\beta\\
         0&0 &0 &0&0 &-2\alpha&0&-2\beta & \gamma
       \end{array}\right)\mathbf{u} = \mathbf{u}_p
\end{align}

\noindent The dashed lines confine a total of nine $3\times 3$ matrices which can be used to reformulate the $2n +1 = 7$ band diagonal system as a block tridiagonal system as shown below.
\begin{align}\label{block_tridiagonal_dashed}
 \left(\begin{array}{c : c : c }
        B_0 & C_0 &0  \\ \hdashline
        A_1 & B_1 & C_1 \\ \hdashline
        0& A_2 & B_2
       \end{array}\right)\mathbf{u} = \mathbf{u}_p
\end{align}

\noindent All entries in eq. \eqref{block_tridiagonal_dashed} are $3\times3$ matrices. 
In the general case where the diffusion equation is discretized and solved on a mesh of size $n\times n$ the resulting block tridiagonal system will consist of $n^2$ matrix entries which all are $n\times n$ matrices.
% In two spatial dimensions the BE discretization will result in a a $2n$ band diagonal matrix of size $n^2\times n^2$ with 5 nonzero bands. 
% This band diagonal matrix can be rewritten as a block tridiagonal matrix with $n\times n$ matrices as entries. 
% A simplified block tridiagonal linear system is shown in eq. \eqref{theory:block_tridiagonal_matrix}
% 
% 
% \begin{align}\label{theory:block_tridiagonal_matrix}
%    \left(\begin{array}{c c c c c c c c c}
%         B_0 & C_0 &0 &0 &0 &0 &0 &0 &0\\
%         A_1 & B_1 & C_1 &0 &0 &0 &0 &0 &0\\
%         0&\ddots & \ddots & 0 & 0 & \ddots &0&0&0\\
%         0 & 0&A_i & B_i & C_i& 0 &  &0&0\\
%         0& \ddots&0&\ddots & \ddots & \ddots & 0 & \ddots &0\\
%          0&0 &0 &0&0 &0&0&A_{n-1} & B_{n-1}
%        \end{array}\right) \left(\begin{array}{c}
%              \mathbf{u}^{n+1}_{0}\\
%              \mathbf{u}^{n+1}_{1}\\
%              \vdots\\
%              \mathbf{u}^{n+1}_{i}\\
%              \vdots\\
%              \mathbf{u}^{n+1}_{n}
%              \end{array}\right) = \mathbf{up}
% \end{align}

One should note that the $n\times n$ matrix $\mathbf u$ which solves the 2D diffusion equation has be rewritten as a vector $\mathbf{u}$ of size $n^2$. \\ 

In order to solve this block tridiagonal linear system Algorithm \ref{theory:tridiag} must be generalized to support matrices as entries in $\mathbf M$. 
Luckily this generalization is trivial and all that is needed is to replace divisions by matrix inverses. 
An updated \emph{pseudo coded} scheme is listed in \eqref{block_tridiag_alg}

\noindent There is a forward substitution
\begin{align}\label{block_tridiag_alg}
 H_0 &= -B_0^{-1}C_0\nonumber \\
 \mathbf{g}_0 &= B_0^{-1}\mathbf{u}_{p0} \nonumber\\
 H_i &= -\left(B_i+A_iH_{i-1}\right)^{-1}C_i \nonumber \\
 \mathbf{g}_i &= \left(B_i+A_iH_{i-1}\right)^{-1}\left(\mathbf{u}_{pi}-A_i\mathbf{g}_{i-1}\right)
 \end{align}
 Followed by a backward substitution
 \begin{align*}
  \mathbf{u}_{n-1} &= \mathbf{g}_{n-1}\nonumber\\
  \mathbf{u}_i &= \mathbf{g}_i + H_i\mathbf{u}_{i+1} \nonumber
 \end{align*}
 
 \noindent It is possible to solve the BE scheme in 3D by the block tridiagonal solver as well. 
 The linear system will be $2n^2 +1$ band diagonal and must first be reduced to a block matrix which is $2n+1$ band diagonal like the one in eq. \eqref{linear_system_BE2D} before it is further reduced to a block tridiagonal form. 
 Matrix entries will be $n^2\times n^2$ matrices.
 
 \subsubsection{Efficiency of the block tridiagonal algorithm}
 
 In general the FE scheme will solve a discrete PDE in $d$ spatial dimensions with $n$ mesh points in each dimension using
 \begin{equation*}
  \mathcal O(n^d)
 \end{equation*}
floating point operations (FLOPs\nomenclature{FLOPs}{Floating Point Operations (per second)}). 
This is the maximum efficiency possible for a PDE in $d$ dimensions (without using some form of symmetry argument) and will be the benchmark for the BE scheme as well.\\

Implicit schemes like BE result in linear systems which, as mentioned, can always be solved by Gaussian elimination by some $\mathcal O(n^3)$ FLOPs for a $n\times n$ matrix. 
In d spatial dimensions the matrices are $n^d\times n^d$, meaning that Gaussian elimination will require $\mathcal{O}(n^{3d})$ FLOPs which is extremely inefficient. 

% Implicit schemes like BE result in linear systems which can be solved by multiplying both sides of the equation with the inverse of the matrix in question. 
% \begin{equation*}
%  \mathbf M^{-1}\mathbf M\mathbf u = \mathbf{M}^{-1}\mathbf{up}
% \end{equation*}
% Inverting a dense matrix demands $\mathcal O(n^3)$ FLOPs for an $n\times n$ matrix. 

The block Tridiagonal solver requires inverting $n$ matrices of size $n^{d-1}\times n^{d-1}$ as well as some matrix-matrix multiplications. 
All of these steps require $\mathcal O(n^{3(d-1)})$ FLOPs, but as long as the coefficient matrix is constant they need only be done once. 
In other words, the $H_i$ terms from \eqref{block_tridiag_alg} can be stored. 
The remaining calculations include three matrix vector multiplications, one of which demands $n$ FLOPs because the matrix is diagonal. 
The other two matrices are dense, so (for now) the remaining two matrix - vector multiplications require $n^2$ FLOPs. 
In summary, the block tridiagonal solver requires $\mathcal{O}(n^{2n-1})$ FLOPs, which is an order more efficient than a general LU decomposition, and in fact any other direct solver the author knows of. 
In 2D the block tridiagonal solver is one order slower than the FE scheme, which means it is still very much usable for small meshes. \\

Another point which should be mentioned is that the block tridiagonal solver is extremely memory efficient compared to other linear system solvers. 
Since the algorithm only loops through the non zero entries, there is no need to assemble the entire matrix. 
For a 2D simulation on a $100\times 100$ mesh, the full assembled matrix will have $100^2\times100^2 = 10^8$ matrix entries, all of which are double precision floats. 
The total size of the matrix in RAM will be $8\cdot10^8$ bytes, which is getting close to the available RAM of a normal computer at  $\sim 8\cdot10^9$ bytes. 
In comparison, storing only the non zero entries requires three block vectors of size $100$ with entries that are $100\times100$ matrices. A total of $24\cdot10^6$ bytes, or a factor $100$ less. 
Effectively, the memory impact is also reduced from $8\cdot n^{2d}$ bytes to $8\cdot n^{2d-1}$ bytes by switching to the block tridiagonal solver. 


\section{Combining micro and macro scale models}

\subsection{The algorithm}
After setting an initial condition and diffusion constant the diffusion problem is solved on both the microscopic and macroscopic meshes and combined into a common solution by the following steps.
\begin{itemize}
 \item The result from previous PDE time step, $\mathbf{u}_p$, is converted to a distribution of random walkers and sent to the RW solver.
 \item The RW solver does a predefined number of micro scale time steps which correspond to one PDE time step.
 \item The result from the RW solver is converted back to a concentration and this replaces the PDE solution, $\mathbf{u}_p$.
 \item $\mathbf{u}_p$ is then used as input to calculate the next time step.
\end{itemize}

\subsection{Conversion between length scales}
As the previous section states, the result from the last PDE time step is converted to a distribution of random walkers. 
This is done by specifying a conversion rate denoted $Hc$ (as was done by Plapp and Karma \cite{plapp2000multiscale}) which is a single, real integer defined by

\begin{equation}\label{theory:Hc_definition}
 C_{ij} = Hc\cdot u_{ij}
\end{equation}

\noindent As before $u_{ij}$ is the solution to the 2D diffusion equation evaluated at $x_0 +i\Delta x$ and $y_0 +j\Delta y$, and $C_{ij}$ is the number of random walkers located in the rectangle defined by $x_i\pm\frac{\Delta x}{2}$ and $y_j\pm\frac{\Delta y}{2}$. 
Walkers are given a random position within this rectangle at the beginning of each PDE time step.

The reason to not simply use the positions from last time step is that the concentration in the RW area could change from one time step to the next. This will result in a different distribution of walkers which must be calculated at each time step. This is illustrated in Figure \ref{theory:concentration_update}.
\begin{figure}[h]
 \centering
%  \includegraphics[scale=0.6]{Figures/concentration_update.eps}
 \caption{}
 \label{theory:concentration_update}
\end{figure}

Equation \eqref{theory:Hc_definition} effectively states that one ''unit`` of the solution $u$ will directly correspond to $Hc$ random walkers. Depending on the application and the units of the PDE solution, $Hc \approx 20-50$.

\subsection{Coupling the models through step length}

To ensure that the $\tau$ time steps performed by the RW solver sums up to one time step for the PDE solver a limitation must be imposed on the RW solver. 
This limitation can only be imposed on the step length of the random walkers, so the task at hand is to relate the step length to the PDE time step, $\Delta t$, and $\tau$. \\
Through an Einstein relation the variance in position is coupled to the diffusion constant.
\begin{equation}\label{theory:einstein_relation}
 \langle\tilde{\Delta x}^2\rangle = 2Dd\tilde{\Delta t}
\end{equation}
Where $\tilde{\Delta t}$ is the time step for the random walkers which is exactly $\tilde{\Delta t} = \frac{\Delta t}{\tau}$. 
Similarly $\tilde{\Delta x}$ is the spatial resolution on the micro scale which is also the step length, $l$, for the random walkers. 
Rewriting equation \eqref{theory:einstein_relation} gives the final expression for the step length

\begin{equation}\label{theory:step_length}
 l = \sqrt{2dD\frac{\Delta t}{\tau}}
\end{equation}

\subsection{Boundary conditions for the random walk}\label{theory:BC_RW}

Because the RW area must correspond to an area on the PDE mesh which has a finite, constant size the RW model must have boundary conditions imposed on it. 
In order to make absolutely sure that no walkers disappear and by extension that the amount of concentration is conserved, perfectly reflecting boundaries are chosen for the RW model. 
Reflecting boundaries correspond to zero flux Neumann boundaries
\begin{equation}
 \frac{\d C}{\d n} = 0
\end{equation}
Neumann boundaries might seem unphysical, but the redistribution of walkers at each PDE time step must also to some extent be considered as a boundary condition in which fluxes are exchanged between the two models. \\
In hindsight, a possibly better choice of boundary conditions for the RW model would have been perfect exchange of flux between the two solvers. 
\begin{equation}
 \frac{\d u}{\d n} = \frac{\d C}{\d n}
\end{equation}
Since time does not allow for this to be implemented and tested it will be left as a reference to possible future work.

\section{Potential problems}\label{problems_and_pitfalls}
 
 This section will list and discuss some of the problems which were encountered while working on the thesis and present the solutions or workarounds.
% \emph{\textcolor{red}{This section will identify and discuss a few obvious difficulties which might arise in this project. As far as possible solutions or workarounds will be presented, but some problems might not be solvable.}}

% \subsubsection{Different timescales}
% The time scales are coupled through the step length of the random walkers which is shown in equation \eqref{steplength}. 
% \begin{equation}\label{steplength}
%  l = \sqrt{2Dd\frac{\Delta t}{\tau}}
% \end{equation}
% 
% Here $\tau$ is the number of time steps taken on the microscopic scale for each time step on the PDE scale, $d$ is the number of spatial dimensions, and $\Delta t$ is the time step on the PDE scale. 


 
\subsubsection{Negative concentration of walkers}
Physically a negative concentration does not make sense, but if an initial condition with negative values is imposed on the system, the software will try to allocate a negative number of walkers. 
Trying to handle this is more of an oddity than anything else, but a workaround has been found. 
If the absolute value of the negative concentration is taken as input to the RW solver and the sign at each mesh point is stored while the RW solver advances the system, the resulting solution can be multiplied by the stored sign to give back a negative concentration. 
The workaround will at least keep the simulation from crashing, but has a fundamental problem. 

Figure \ref{theory:signmap_illustration}a shows how a negative concentration is converted to a positive concentration by taking the absolute value. This results in an abrupt change where the original function, $u(x,t)$, crossed the x-axis. Seeing as a diffusion process very efficiently evens out abrupt changes, the point where $u(x,t)$ crossed the x-axis will get an increased value. As Figure \ref{theory:signmap_illustration}b shows, this point should remain equal to zero, but does not.

  
\begin{figure}[H]
\centering
\begin{subfigure}[t!]{0.48\textwidth}
 \includegraphics[width=\textwidth]{Figures/signmap_illustration_pt1.eps}
 \caption{}
\end{subfigure}
\begin{subfigure}[t!]{0.48\textwidth}
 \includegraphics[width=\textwidth]{Figures/signmap_illustration_pt2.eps}
 \caption{}
\end{subfigure}
\caption[Workaround for negative concentrations, illustration]{An illustration of the proposed workaround for negative concentrations and an illustration of how it performs (b). The fundamental problem is that a diffusion process will even out discontinuities such as the one in (a). The result of this evening out is shown in (b) as an increase in concentration in the middle mesh point which should remain equal to zero.}
\label{theory:signmap_illustration}
 \end{figure}

\subsubsection{Smooth solutions}

 A diffusion process is very effective when it comes to dampening fast fluctuations, and so any solution of the diffusion equation will be smooth. 
 Random walks are stochastic and though a RW will describe a diffusion process it will fluctuate around the solution. 
 In this case a dilemma arises; on the one hand there is the smoothness of the solution to consider, on the other hand the stochastic term was introduced believing that it adds detail to the model. 
This is partly the reason for solving the RW model before the PDE model. 
The fluctuations will be reduced by the PDE solver and the solution, though still fluctuating, will be smoother. \\

Section \ref{other_possible_coupling_methods} describes some other methods which were attempted in order to reduce the fluctuations from the RW model.

\subsubsection{Number of time steps on the random walk level}
The reason for introducing a random walk model in the diffusion solver was to include an area with more advanced dynamics than regular diffusion on a smaller time-, and length scale. 
This means that the RW solver must take $\tau$ time steps for each PDE time step. 
$\tau$ could in principle be any integer, but should be chosen large enough so the central limit theorem is satisfied. 
This usually means $\tau\geq50$. \\
Alternatively $\tau$ can be calculated from the assumption that the error term with respect to time arising from the RW model is proportional to the square root of the time step, 

\begin{equation}
 \epsilon \propto \sqrt{\tilde{\Delta t}}.
\end{equation}
This assumption comes from the fact that the step length is proportional to the square root of the time step.
In order to make the error of the same size as the error from the PDE model we use 
\begin{align*}
 \tilde{\Delta t} &= \frac{\Delta t}{\tau} \\
 \implies \epsilon &\propto \sqrt{\frac{\Delta t}{\tau}} \\
 \mathcal{O}(\Delta t) &\geq \sqrt{\frac{\Delta t}{\tau}} \\
 \implies \tau &\geq \frac{1}{\Delta t}
\end{align*}
\noindent Note that the error associated with the number of walkers introduced is of much higher significance than the error from the time step.

\section{Other possible coupling methods}\label{other_possible_coupling_methods}

Running the solution from the PDE model through a RW model gives two solutions for a part of the mesh. 
As it turns out it is sufficient to simply replace the PDE solution with the RW solution in this area, which is nice seeing as it is the simplest possible method. 
There are, however, some other ways to do this which have been tested, and found inadequate or simply been disregarded. Three of these are mentioned below. 

\subsubsection{Averaging the two solutions}
 Taking the arithmetic mean of the two solutions will reduce the magnitude of the fluctuations, as shown in Figure \ref{theory:average}. 
 This method is simple and reduces fluctuations by approximately one half (approximately because fluctuations will be present after the first time step). 
 \begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/average.eps}
  \caption[Idea behind averaging solutions]{This plot shows the PDE solution before and after the stochastic solver has been called, and the arithmetic mean of the two solutions which has fluctuations of smaller amplitude.}
  \label{theory:average}
 \end{figure}

 The reason an average was scrapped is simply that equally satisfactory results were achieved by only replacing the solution.
 
\subsubsection{Polynomial regression}

 In order to smoothen out the solution from the RW model a polynomial regression function was tested. 
 The principle was to look at the solution from the RW model as a set of data points which have some dependency on their position on the PDE mesh. 
 Figure \ref{theory:polyreg} shows how this method worked for a slightly exaggerated case. 
 \begin{figure}[h]
 \centering
 \includegraphics[width=0.7\textwidth]{Figures/polyreg.png}
 \caption[Combination by polynomial regression]{This plot shows the PDE solution before and after the stochastic solver has been called, and a polynomial regression of degree 6 using $C(x,t)$ as data points. }
%  As is apparent there is no energy conservation when using polynomial regression}
 \label{theory:polyreg}
 \end{figure}
 This approach turns out to have two problems; first and foremost the amount of concentration is no longer conserved, secondly the endpoints are usually way off.
 
 \subsubsection{Cubic spline interpolation}
 Doing an interpolation will force the combined solution to be equal to the RW solution at some mesh points. In practice this means choosing which points are correct, and which are not. The author has not found any reasonable way to do this. 
 
 In addition, cubic spline interpolation has the same problem as polynomial regression when it comes to conserving energy.
